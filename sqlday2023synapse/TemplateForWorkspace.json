{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "sqlday2023synapse"
		},
		"sqlday2023synapse-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'sqlday2023synapse-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:sqlday2023synapse.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"DemoLearn_sasUri": {
			"type": "secureString",
			"metadata": "Secure string for 'sasUri' of 'DemoLearn'"
		},
		"DemoLearn_properties_typeProperties_sasToken_secretName": {
			"type": "string",
			"defaultValue": "DemoLearn"
		},
		"SQLDay_AKV_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://sqlday2023akv.vault.azure.net/"
		},
		"sqlday2023synapse-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://sqlday2023.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/01 Load Dimensions - init')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "01 Create Actor",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "01 Create Actor",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "smallpool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": true,
								"spark.dynamicAllocation.minExecutors": 1,
								"spark.dynamicAllocation.maxExecutors": 2
							},
							"driverSize": "Small",
							"numExecutors": 1
						}
					},
					{
						"name": "02 Create Geo",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "01 Create Actor",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "02 Create Geo",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "smallpool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": true,
								"spark.dynamicAllocation.minExecutors": 1,
								"spark.dynamicAllocation.maxExecutors": 2
							},
							"driverSize": "Small",
							"numExecutors": 1
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "Initial Load"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/01 Create Actor')]",
				"[concat(variables('workspaceId'), '/bigDataPools/smallpool')]",
				"[concat(variables('workspaceId'), '/notebooks/02 Create Geo')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DemoLearn')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"sasUri": "[parameters('DemoLearn_sasUri')]",
					"sasToken": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "SQLDay_AKV",
							"type": "LinkedServiceReference"
						},
						"secretName": "[parameters('DemoLearn_properties_typeProperties_sasToken_secretName')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/SQLDay_AKV')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQLDay_AKV')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('SQLDay_AKV_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sqlday2023synapse-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('sqlday2023synapse-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sqlday2023synapse-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('sqlday2023synapse-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/broze vs raw compare')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n     COUNT(*)\nFROM\n    OPENROWSET(\n        BULK 'https://sqlday2023.dfs.core.windows.net/bronze/customer/*.parquet',\n        FORMAT = 'PARQUET'\n    ) AS [result]\nGO\nSELECT\n    COUNT(*) \nFROM\n    OPENROWSET(\n        BULK 'https://sqlday2023.dfs.core.windows.net/raw/customer/*.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/event source data example')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://sqlday2023.dfs.core.windows.net/bronze/202305/20230502.export.CSV',\n        FORMAT = 'CSV',\n        FIELDTERMINATOR = '\\t',\n        PARSER_VERSION = '2.0'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Spark job sample')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.3",
				"language": "python",
				"scanFolder": false,
				"jobProperties": {
					"name": "Spark job sample",
					"file": "abfss://synapse@sqlday2023.dfs.core.windows.net/spark_job.py",
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "98762765-218f-44b3-b6cd-94e2f81e1aca",
						"spark.synapse.context.sjdname": "Spark job sample"
					},
					"args": [
						"abfss://raw@sqlday2023.dfs.core.windows.net/AWInternetSales.csv",
						"abfss://bronze@sqlday2023.dfs.core.windows.net/AWorks"
					],
					"jars": [],
					"pyFiles": [
						""
					],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01 Create Actor')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "02 Load to Delta"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "396d7f54-308d-4285-ae1e-245631d392dc"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\r\n",
							"\r\n",
							"from pyspark.sql.functions import monotonically_increasing_id\r\n",
							"from delta.tables import DeltaTable"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"src_location = 'abfss://bronze@sqlday2023.dfs.core.windows.net/gdeltevents/20*/*/gdelt_events.parquet'\r\n",
							"dst_location = 'abfss://silver@sqlday2023.dfs.core.windows.net/gdeltevents/actor'"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df_all_events = spark.read.parquet(src_location)\r\n",
							"\r\n",
							"actor_columns1 = ['Actor1Code'\r\n",
							",'Actor1Name'\r\n",
							",'Actor1CountryCode'\r\n",
							",'Actor1KnownGroupCode'\r\n",
							",'Actor1EthnicCode'\r\n",
							",'Actor1Religion1Code'\r\n",
							",'Actor1Religion2Code'\r\n",
							",'Actor1Type1Code'\r\n",
							",'Actor1Type2Code'\r\n",
							",'Actor1Type3Code']\r\n",
							"actor_columns2 = ['Actor2Code'\r\n",
							",'Actor2Name'\r\n",
							",'Actor2CountryCode'\r\n",
							",'Actor2KnownGroupCode'\r\n",
							",'Actor2EthnicCode'\r\n",
							",'Actor2Religion1Code'\r\n",
							",'Actor2Religion2Code'\r\n",
							",'Actor2Type1Code'\r\n",
							",'Actor2Type2Code'\r\n",
							",'Actor2Type3Code']\r\n",
							"\r\n",
							"df_actor1 = df_all_events.select(actor_columns1)\r\n",
							"df_actor2 = df_all_events.select(actor_columns2)\r\n",
							"\r\n",
							"df_actor1 = df_actor1.toDF('ActorCode'\r\n",
							",'ActorName'\r\n",
							",'ActorCountryCode'\r\n",
							",'ActorKnownGroupCode'\r\n",
							",'ActorEthnicCode'\r\n",
							",'ActorReligion1Code'\r\n",
							",'ActorReligion2Code'\r\n",
							",'ActorType1Code'\r\n",
							",'ActorType2Code'\r\n",
							",'ActorType3Code')\r\n",
							"df_actor2 = df_actor2.toDF('ActorCode'\r\n",
							",'ActorName'\r\n",
							",'ActorCountryCode'\r\n",
							",'ActorKnownGroupCode'\r\n",
							",'ActorEthnicCode'\r\n",
							",'ActorReligion1Code'\r\n",
							",'ActorReligion2Code'\r\n",
							",'ActorType1Code'\r\n",
							",'ActorType2Code'\r\n",
							",'ActorType3Code')\r\n",
							"\r\n",
							"df_actor = df_actor1.union(df_actor2).dropDuplicates(['ActorCode'\r\n",
							",'ActorName'])\r\n",
							"\r\n",
							"display(df_actor.limit(10))"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"## full reload\r\n",
							"delta_table = DeltaTable.forPath(spark, dst_location)\r\n",
							"delta_table.delete()\r\n",
							"delta_table.vacuum()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"df_actor_wkey = df_actor.withColumn('ActorID', monotonically_increasing_id())\r\n",
							"df_actor_wkey.write.option(\"path\", dst_location).format(\"delta\").mode('overwrite').save()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\"\"\" df_events = df_events.toDF('GlobalEventID'\r\n",
							",'Day'\r\n",
							",'MonthYear'\r\n",
							",'Year'\r\n",
							",'FractionDate'\r\n",
							",'Actor1Code'\r\n",
							",'Actor1Name'\r\n",
							",'Actor1CountryCode'\r\n",
							",'Actor1KnownGroupCode'\r\n",
							",'Actor1EthnicCode'\r\n",
							",'Actor1Religion1Code'\r\n",
							",'Actor1Religion2Code'\r\n",
							",'Actor1Type1Code'\r\n",
							",'Actor1Type2Code'\r\n",
							",'Actor1Type3Code'\r\n",
							",'Actor2Code'\r\n",
							",'Actor2Name'\r\n",
							",'Actor2CountryCode'\r\n",
							",'Actor2KnownGroupCode'\r\n",
							",'Actor2EthnicCode'\r\n",
							",'Actor2Religion1Code'\r\n",
							",'Actor2Religion2Code'\r\n",
							",'Actor2Type1Code'\r\n",
							",'Actor2Type2Code'\r\n",
							",'Actor2Type3Code'\r\n",
							",'IsRootEvent'\r\n",
							",'EventCode'\r\n",
							",'EventBaseCode'\r\n",
							",'EventRootCode'\r\n",
							",'QuadClass'\r\n",
							",'GoldsteinScale'\r\n",
							",'NumMentions'\r\n",
							",'NumSources'\r\n",
							",'NumArticles'\r\n",
							",'AvgTone'\r\n",
							",'Actor1Geo_Type'\r\n",
							",'Actor1Geo_Fullname'\r\n",
							",'Actor1Geo_CountryCode'\r\n",
							",'Actor1Geo_ADM1Code'\r\n",
							",'Actor1GeoID'\r\n",
							",'Actor1Geo_Long'\r\n",
							",'Actor1Geo_FeatureID'\r\n",
							",'Actor2Geo_Type'\r\n",
							",'Actor2Geo_Fullname'\r\n",
							",'Actor2Geo_CountryCode'\r\n",
							",'Actor2Geo_ADM1Code'\r\n",
							",'Actor2Geo_Lat'\r\n",
							",'Actor2Geo_Long'\r\n",
							",'Actor2Geo_FeatureID'\r\n",
							",'ActionGeo_Type'\r\n",
							",'ActionGeo_Fullname'\r\n",
							",'ActionGeo_CountryCode'\r\n",
							",'ActionGeo_ADM1Code'\r\n",
							",'ActionGeo_Lat'\r\n",
							",'ActionGeo_Long'\r\n",
							",'ActionGeo_FeatureID'\r\n",
							",'DATEADDED'\r\n",
							",'SOURCEURL') \"\"\"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/02 Create Geo')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "02 Load to Delta"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "28b70292-3bb7-4dd5-8f14-f78180b78723"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\r\n",
							"\r\n",
							"from pyspark.sql.functions import monotonically_increasing_id, col, regexp_replace\r\n",
							"from delta.tables import DeltaTable"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"src_location = 'abfss://bronze@sqlday2023.dfs.core.windows.net/gdeltevents/20*/*/gdelt_events.parquet'\r\n",
							"dst_location = 'abfss://silver@sqlday2023.dfs.core.windows.net/gdeltevents/geo'"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df_all_events = spark.read.parquet(src_location)\r\n",
							"\r\n",
							"geo_columns1 = ['Actor1Geo_Type'\r\n",
							",'Actor1Geo_Fullname'\r\n",
							",'Actor1Geo_CountryCode'\r\n",
							",'Actor1Geo_ADM1Code'\r\n",
							",'Actor1Geo_Lat'\r\n",
							",'Actor1Geo_Long'\r\n",
							",'Actor1Geo_FeatureID']\r\n",
							"geo_columns2 = ['Actor2Geo_Type'\r\n",
							",'Actor2Geo_Fullname'\r\n",
							",'Actor2Geo_CountryCode'\r\n",
							",'Actor2Geo_ADM1Code'\r\n",
							",'Actor2Geo_Lat'\r\n",
							",'Actor2Geo_Long'\r\n",
							",'Actor2Geo_FeatureID']\r\n",
							"geo_columns3 = ['ActionGeo_Type'\r\n",
							",'ActionGeo_Fullname'\r\n",
							",'ActionGeo_CountryCode'\r\n",
							",'ActionGeo_ADM1Code'\r\n",
							",'ActionGeo_Lat'\r\n",
							",'ActionGeo_Long'\r\n",
							",'ActionGeo_FeatureID']\r\n",
							"\r\n",
							"df_geo1 = df_all_events.select(geo_columns1)\r\n",
							"df_geo2 = df_all_events.select(geo_columns2)\r\n",
							"df_geo3 = df_all_events.select(geo_columns3)\r\n",
							"\r\n",
							"df_geo1 = df_geo1.toDF('Geo_Type'\r\n",
							",'Geo_Fullname'\r\n",
							",'Geo_CountryCode'\r\n",
							",'Geo_ADM1Code'\r\n",
							",'Geo_Lat'\r\n",
							",'Geo_Long'\r\n",
							",'Geo_FeatureID')\r\n",
							"df_geo2 = df_geo2.toDF('Geo_Type'\r\n",
							",'Geo_Fullname'\r\n",
							",'Geo_CountryCode'\r\n",
							",'Geo_ADM1Code'\r\n",
							",'Geo_Lat'\r\n",
							",'Geo_Long'\r\n",
							",'Geo_FeatureID')\r\n",
							"df_geo3 = df_geo3.toDF('Geo_Type'\r\n",
							",'Geo_Fullname'\r\n",
							",'Geo_CountryCode'\r\n",
							",'Geo_ADM1Code'\r\n",
							",'Geo_Lat'\r\n",
							",'Geo_Long'\r\n",
							",'Geo_FeatureID')\r\n",
							"\r\n",
							"\r\n",
							"df_geo_all = df_geo1.union(df_geo2).union(df_geo3)\r\n",
							"df_geo_all = df_geo_all.withColumn('Geo_Lat', regexp_replace(col('Geo_Lat'), '[^0-9\\\\-.]', '')).withColumn('Geo_Long', regexp_replace(col('Geo_Long'), '[^0-9\\\\-.]', ''))\r\n",
							"df_geo = df_geo_all.dropDuplicates(['Geo_ADM1Code'\r\n",
							",'Geo_Lat'\r\n",
							",'Geo_Long'\r\n",
							",'Geo_FeatureID'])\r\n",
							"\r\n",
							"display(df_geo.limit(10))"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"## full reload\r\n",
							"delta_table = DeltaTable.forPath(spark, dst_location)\r\n",
							"delta_table.delete()\r\n",
							"delta_table.vacuum()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"df_geo_wkey = df_geo.withColumn('GeoID', monotonically_increasing_id())\r\n",
							"df_geo_wkey.write.option(\"path\", dst_location).format(\"delta\").mode('overwrite').save()"
						],
						"outputs": [],
						"execution_count": 5
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Data Engineering Spark Pool')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "04 Azure Synapse Analytics"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8530d5d5-28bc-4d08-a351-12c94d856cc1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
							"import os\n",
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run \"04 Azure Synapse Analytics/configuration/configuration_paramaters_bronze_silver\""
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_configuration():\n",
							"    return PARAMETERS.DATALAKE, PARAMETERS.BRONZE_CONTAINER, PARAMETERS.SILVER_CONTAINER, PARAMETERS.BRONZE_SASKEY, PARAMETERS.SILVER_SASKEY\n",
							""
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _prepare_paths_for_data_processing (account: str, container: str, folder: str):\n",
							"\n",
							"    account_for_abfss = account.split('://')[-1].replace(\"blob\",\"dfs\")\n",
							"        \n",
							"    path_to_blob = \"abfss://\" + container + \"@\" + account_for_abfss+\"/\"+ folder\n",
							"    return account_for_abfss, path_to_blob"
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"account , bronze_container, silver_container, bronze_sas_token, silver_sas_token = get_configuration()\n",
							""
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"account_for_abfss,  path_to_orders = _prepare_paths_for_data_processing(account, bronze_container, \"orders\")\n",
							"account_for_abfss,  path_to_customer = _prepare_paths_for_data_processing(account, bronze_container, \"customer\")\n",
							"account_for_abfss_silver,  path_to_customer_silver = _prepare_paths_for_data_processing(account, silver_container, \"customer/delta\")\n",
							"account_for_abfss_silver,  path_to_orders_silver = _prepare_paths_for_data_processing(account, silver_container, \"orders/delta\")\n",
							"account_for_abfss_silver,  path_to_mktsegment_silver = _prepare_paths_for_data_processing(account, silver_container, \"mktsegment/delta\")\n",
							"account_for_abfss_silver,  path_to_orderpriority_silver = _prepare_paths_for_data_processing(account, silver_container, \"orderpriority/delta\")"
						],
						"outputs": [],
						"execution_count": 145
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"account_for_abfss,  path_to_orders, path_to_customer, path_to_customer_silver, path_to_orders_silver, path_to_mktsegment_silver, path_to_orderpriority_silver"
						],
						"outputs": [],
						"execution_count": 146
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_orders = spark.read.load(path_to_orders, format='parquet')"
						],
						"outputs": [],
						"execution_count": 164
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(df_orders.limit(10))"
						],
						"outputs": [],
						"execution_count": 165
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_customer = spark.read.load(path_to_customer, format='parquet')"
						],
						"outputs": [],
						"execution_count": 166
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(df_customer.limit(10))"
						],
						"outputs": [],
						"execution_count": 167
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import lit\n",
							"from datetime import datetime\n",
							"\n",
							"now = datetime.now()\n",
							"dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
							"\n",
							"df_customer = df_customer.withColumn(\"##_RecordLoaded\", lit(dt_string))\n",
							"df_customer = df_customer.withColumn(\"C_MKTSEGMENT_ID\", lit(0))\n",
							"\n",
							"df_orders = df_orders.withColumn(\"O_ORDERPRIORITY_ID\", lit(0))"
						],
						"outputs": [],
						"execution_count": 168
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(df_customer.limit(10))"
						],
						"outputs": [],
						"execution_count": 169
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"list_segment = df_customer.select('C_MKTSEGMENT').distinct().collect()"
						],
						"outputs": [],
						"execution_count": 170
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_mktsegment = spark.createDataFrame(list_segment)\n",
							""
						],
						"outputs": [],
						"execution_count": 171
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(df_mktsegment)"
						],
						"outputs": [],
						"execution_count": 172
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"list_order_priority = df_orders.select('O_ORDERPRIORITY').distinct().collect()\n",
							"df_order_priority= spark.createDataFrame(list_order_priority)"
						],
						"outputs": [],
						"execution_count": 173
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import functions as sf\n",
							"from pyspark.sql.window import Window\n",
							"\n",
							"df_mktsegment = df_mktsegment.withColumn(\"C_MKTSEGMENT_ID\", sf.row_number().over(Window.orderBy(sf.col(\"C_MKTSEGMENT\"))))\n",
							"df_order_priority = df_order_priority.withColumn(\"O_ORDERPRIORITY_ID\", sf.row_number().over(Window.orderBy(sf.col(\"O_ORDERPRIORITY\"))))"
						],
						"outputs": [],
						"execution_count": 174
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(df_mktsegment)"
						],
						"outputs": [],
						"execution_count": 175
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"db = \"SilverDatabase\"\n",
							"spark.sql(f\"DROP DATABASE IF EXISTS {db} CASCADE\")\n",
							"spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db}\")\n",
							"spark.sql(f\"USE {db}\")"
						],
						"outputs": [],
						"execution_count": 176
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_mktsegment.write .format(\"delta\") .mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(path_to_mktsegment_silver)\n",
							"df_order_priority.write .format(\"delta\") .mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(path_to_orderpriority_silver)\n",
							"df_customer .write .format(\"delta\") .mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(path_to_customer_silver)\n",
							"df_orders.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(path_to_orders_silver)"
						],
						"outputs": [],
						"execution_count": 177
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _drop_table (folder: str):\n",
							"    \n",
							"    str = f\"DROP TABLE IF EXISTS {folder}; \"\n",
							"    spark.sql(str)\n",
							"    "
						],
						"outputs": [],
						"execution_count": 178
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"def _create_table (folder: str, path_to_silver:str):\n",
							"    \n",
							"    _drop_table(folder)\n",
							"    \n",
							"    str = f\"CREATE TABLE {folder} \\\n",
							"    USING delta \\\n",
							"    location '{path_to_silver}'\"\n",
							"\n",
							"    spark.sql(str)"
						],
						"outputs": [],
						"execution_count": 179
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"_create_table (\"mkt_segment\", path_to_mktsegment_silver)\n",
							"_create_table (\"order_priority\", path_to_orderpriority_silver)\n",
							"_create_table (\"customer\", path_to_customer_silver)\n",
							"_create_table (\"orders\", path_to_orders_silver)\n",
							""
						],
						"outputs": [],
						"execution_count": 180
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"select * from order_priority"
						],
						"outputs": [],
						"execution_count": 182
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"MERGE INTO customer dest\n",
							"USING mkt_segment src\n",
							"ON src.C_MKTSEGMENT = dest.C_MKTSEGMENT\n",
							"When Matched then\n",
							"Update \n",
							"    SET C_MKTSEGMENT_ID = src.C_MKTSEGMENT_ID\n",
							""
						],
						"outputs": [],
						"execution_count": 186
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"ALTER TABLE customer SET TBLPROPERTIES (\n",
							"   'delta.columnMapping.mode' = 'name',\n",
							"   'delta.minReaderVersion' = '2',\n",
							"   'delta.minWriterVersion' = '5')"
						],
						"outputs": [],
						"execution_count": 190
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _drop_column (table: str, column: str):\n",
							"    str = f\"ALTER TABLE {table} \\\n",
							"    DROP column {column}\"\n",
							"\n",
							"    spark.sql(str)\n",
							""
						],
						"outputs": [],
						"execution_count": 188
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"_drop_column(\"customer\",\"C_MKTSEGMENT\")"
						],
						"outputs": [],
						"execution_count": 191
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"select * from customer"
						],
						"outputs": [],
						"execution_count": 192
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"MERGE INTO orders dest\n",
							"USING order_priority src\n",
							"ON src.O_ORDERPRIORITY = dest.O_ORDERPRIORITY\n",
							"When Matched then\n",
							"Update \n",
							"    SET O_ORDERPRIORITY_ID = src.O_ORDERPRIORITY_ID"
						],
						"outputs": [],
						"execution_count": 193
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"ALTER TABLE orders SET TBLPROPERTIES (\n",
							"   'delta.columnMapping.mode' = 'name',\n",
							"   'delta.minReaderVersion' = '2',\n",
							"   'delta.minWriterVersion' = '5')"
						],
						"outputs": [],
						"execution_count": 195
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"_drop_column(\"orders\",\"O_ORDERPRIORITY\")"
						],
						"outputs": [],
						"execution_count": 196
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"some other transformations"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"class TransformationException(Exception):\n",
							"    pass"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"class MustNotModifyProtectedColumnsException (TransformationException):\n",
							"    pass"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"PROTECTED_COLUMNS: set[str] = {\n",
							"    \"##_RecordLoaded\",\n",
							"    \"_source\",\n",
							"    \"_errCode\"\n",
							"}"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _check_protected_column (column_name: str):\n",
							"    if column_name in PROTECTED_COLUMNS:\n",
							"        raise MustNotModifyProtectedColumnsException\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def add_column_to_target_as_copy (df: DataFrame, sourceColumn: str, targetColumn: str) -> DataFrame:\n",
							"    _check_protected_column(targetColumn)\n",
							"\n",
							"    if targetColumn in df.schema.names:\n",
							"        df.withColumn(column=targetColumn, col=F.col(sourceColumn))\n",
							"\n",
							"    return df"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def rename_column (df: DataFrame, before: str, after: str)-> DataFrame:\n",
							"    _check_protected_column(before)\n",
							"    _check_protected_column(after)\n",
							"\n",
							"    columns = df.columns\n",
							"\n",
							"    if before not in columns:\n",
							"        print (f'')\n",
							"    if after in columns:\n",
							"        print (f'')\n",
							"\n",
							"    return df.withColumnRenamed(before, after)    \n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def drop_column (df: DataFrame, columnName: str)-> DataFrame:\n",
							"    _check_protected_column(column_name)\n",
							"\n",
							"    return df.drop(columnName)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LoadData do not use on sqlday')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "other"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1b2c7194-d592-463b-8918-0b00d56f3e25"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install zipfile36"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\r\n",
							"import requests\r\n",
							"import pandas as pd\r\n",
							"import zipfile\r\n",
							"import io\r\n",
							"from datetime import datetime, timedelta"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sas_token = f'sp=racwdlm&st=2023-04-22T21:20:01Z&se=2023-05-31T05:20:01Z&spr=https&sv=2021-12-02&sr=c&sig=7Hb04rR6uzijvOjHdc70Xt9vyt4iEPx5i3AnoE20f%2Fg%3D'\r\n",
							"account = 'https://sqlday2023.blob.core.windows.net'\r\n",
							"url_base = 'http://data.gdeltproject.org/events/'\r\n",
							"container = 'data'\r\n",
							"unizpped_container = 'data/unizpped'\r\n",
							"file_extension = 'csv'"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_blob_service_client_sas( account, sas_token):\r\n",
							"    account_url = account\r\n",
							"    credential = sas_token\r\n",
							"    blob_service_client = BlobServiceClient(account_url, credential=credential)\r\n",
							"\r\n",
							"    return blob_service_client"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def send_data_to_blob (container_client: ContainerClient, fileName:str):\n",
							"    url = url_base+fileName\n",
							"\n",
							"\n",
							"    print (f'url: {url}')\n",
							"\n",
							"    file = fileName.split('.')[0]\n",
							"    file_to_save = file +'.'+file_extension\n",
							"\n",
							"    r = requests.get(url, allow_redirects=True)\n",
							"    zipDocument = zipfile.ZipFile(io.BytesIO(r.content))  \n",
							"    \n",
							"    listOfFileNames = zipDocument.namelist()\n",
							"\n",
							"    for file in listOfFileNames:\n",
							"        tmp = zipDocument.extract(file)\n",
							"        print (f'file: {file}, p={tmp}')\n",
							"\n",
							"        f = open (tmp,\"r\")\n",
							"        content = f.read()\n",
							"\n",
							"        blob_client = container_client.upload_blob(name=file, data=content, overwrite=True, blob_type=\"BlockBlob\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _create_nested_container_client(blob_service_client: BlobServiceClient, container_name: str, subfolder: str):\n",
							"    new_name_for_container = container_name + \"/\" + subfolder[0:6]\n",
							"    return blob_service_client.get_container_client(container=new_name_for_container)\n",
							"    \n",
							"    "
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_blob_file_1979_2005(blob_service_client: BlobServiceClient, container_name):\r\n",
							"    container_client = blob_service_client.get_container_client(container=container_name)\r\n",
							"\r\n",
							"    for dt in range(1979,2006,1):\r\n",
							"        dt_str = str(dt)+'.zip'\r\n",
							"        send_data_to_blob(container_client,dt_str)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_blob_file_200601_201303(blob_service_client: BlobServiceClient, container_name: str):\n",
							"\n",
							"    for dt in pd.period_range(start='2006-01-01', end='2013-03-01', freq='M'):\n",
							"        dt_str = str(dt).replace(\"-\",\"\")+'.zip'\n",
							"\n",
							"        container_client = _create_nested_container_client(blob_service_client, container_name, dt_str)\n",
							"        \n",
							"        send_data_to_blob(container_client,dt_str)"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_blob_file_20130401_20230331(blob_service_client: BlobServiceClient, container_name):\n",
							"    container_client = blob_service_client.get_container_client(container=container_name) \n",
							"\n",
							"    for dt in pd.period_range(start='2014-02-01', end='2023-03-31', freq='D'):\n",
							"        dt_str = str(dt).replace(\"-\",\"\")+'.export.CSV.zip'\n",
							"\n",
							"        container_client = _create_nested_container_client(blob_service_client, container_name, dt_str)\n",
							"        \n",
							"        send_data_to_blob(container_client,dt_str)"
						],
						"outputs": [],
						"execution_count": 83
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_blob_file_for_dates(blob_service_client: BlobServiceClient, container_name: str, date_start:str, date_end:str):\n",
							"    '''date_start and date_end in format YYYY-MM-DD'''\n",
							"\n",
							"    for dt in pd.period_range(start=date_start, end=date_end, freq='D'):\n",
							"        dt_str = str(dt).replace(\"-\",\"\")+'.export.CSV.zip'\n",
							"\n",
							"        container_client = _create_nested_container_client(blob_service_client, container_name, dt_str)\n",
							"        \n",
							"        send_data_to_blob(container_client,dt_str)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_last_X_days(blob_service_client: BlobServiceClient, container_name: str, days: int):\n",
							"    end_date = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
							"    start_date = (datetime.today() - timedelta(days=days)).strftime(\"%Y-%m-%d\")\n",
							"\n",
							"    upload_blob_file_for_dates(blob_service_client, container_name,start_date,end_date )\n",
							"\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"bsc = get_blob_service_client_sas(account,sas_token)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"upload_blob_file_for_dates(bsc,unizpped_container, '2023-04-20','2023-04-29')"
						],
						"outputs": [],
						"execution_count": 116
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"upload_last_X_days(bsc,unizpped_container,7)"
						],
						"outputs": [],
						"execution_count": 13
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ML on Spark Pool')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "04 Azure Synapse Analytics"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6bc0202e-3ab5-4629-b699-bcd1824434f1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": true
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%%configure -f\n",
							"{\n",
							"  \"name\": \"synapseml\",\n",
							"  \"conf\": {\n",
							"      \"spark.dynamicAllocation.enabled\": \"false\",\n",
							"      \"spark.jars.repositories\": \"https://mmlspark.azureedge.net/maven\"\n",
							"  }\n",
							"}"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from synapse.ml.core.platform import *"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = (\n",
							"    spark.read.format(\"csv\")\n",
							"    .option(\"header\", True)\n",
							"    .option(\"inferSchema\", True)\n",
							"    .load(\n",
							"        \"wasbs://publicwasb@mmlspark.blob.core.windows.net/heart_disease_prediction_data.csv\"\n",
							"    )\n",
							")\n",
							"print(\"records read: \" + str(df.count()))\n",
							"print(\"Schema: \")\n",
							"df.printSchema()"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(df)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"train, test = df.randomSplit([0.85, 0.15], seed=1)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from synapse.ml.vw import VowpalWabbitFeaturizer\n",
							"\n",
							"featurizer = VowpalWabbitFeaturizer(inputCols=df.columns[:-1], outputCol=\"features\")\n",
							"train_data = featurizer.transform(train)[\"target\", \"features\"]\n",
							"test_data = featurizer.transform(test)[\"target\", \"features\"]"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(train_data.groupBy(\"target\").count())"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from synapse.ml.vw import VowpalWabbitClassifier\n",
							"\n",
							"model = VowpalWabbitClassifier(\n",
							"    numPasses=20, labelCol=\"target\", featuresCol=\"features\"\n",
							").fit(train_data)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"predictions = model.transform(test_data)\n",
							"display(predictions)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from synapse.ml.train import ComputeModelStatistics\n",
							"\n",
							"metrics = ComputeModelStatistics(\n",
							"    evaluationMetric=\"classification\", labelCol=\"target\", scoredLabelsCol=\"prediction\"\n",
							").transform(predictions)\n",
							"display(metrics)"
						],
						"outputs": [],
						"execution_count": 18
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Raw  data ingestion')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "01 Load raw data"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "32f2ff21-d433-4e1e-9d27-2e6051aa68ea"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\r\n",
							"import requests\r\n",
							"import pandas as pd\r\n",
							"import zipfile\r\n",
							"import io\r\n",
							"from datetime import datetime, timedelta"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#(bronze)sas_token = f'sp=racwdlmeop&st=2023-05-03T11:20:20Z&se=2023-05-31T19:20:20Z&spr=https&sv=2021-12-02&sr=c&sig=SITj%2FKFPa%2FTLJgawl6TeqABXjTpimz9WcH62TPyxKss%3D'\r\n",
							"sas_token = f'sp=racwdlmeop&st=2023-05-03T21:02:46Z&se=2023-05-31T05:02:46Z&spr=https&sv=2021-12-02&sr=c&sig=8CY7pN26Rsto5wDJoYUM4RzofvxNaozEwpZWARTqVs0%3D'\r\n",
							"account = 'https://sqlday2023.blob.core.windows.net'\r\n",
							"url_base = 'http://data.gdeltproject.org/events/'\r\n",
							"unizpped_container = 'raw'\r\n",
							"file_extension = 'csv'"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_blob_service_client_sas( account, sas_token):\r\n",
							"    account_url = account\r\n",
							"    credential = sas_token\r\n",
							"    blob_service_client = BlobServiceClient(account_url, credential=credential)\r\n",
							"\r\n",
							"    return blob_service_client"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def send_data_to_blob (container_client: ContainerClient, fileName:str):\n",
							"    url = url_base+fileName\n",
							"\n",
							"\n",
							"    print (f'url: {url}')\n",
							"\n",
							"    file = fileName.split('.')[0]\n",
							"    file_to_save = file +'.'+file_extension\n",
							"\n",
							"    r = requests.get(url, allow_redirects=True)\n",
							"    zipDocument = zipfile.ZipFile(io.BytesIO(r.content))  \n",
							"    \n",
							"    listOfFileNames = zipDocument.namelist()\n",
							"\n",
							"    for file in listOfFileNames:\n",
							"        tmp = zipDocument.extract(file)\n",
							"        print (f'file: {file}, p={tmp}')\n",
							"\n",
							"        f = open (tmp,\"r\")\n",
							"        content = f.read()\n",
							"\n",
							"        blob_client = container_client.upload_blob(name=file, data=content, overwrite=True, blob_type=\"BlockBlob\")"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _create_nested_container_client(blob_service_client: BlobServiceClient, container_name: str, subfolder: str):\n",
							"    new_name_for_container = container_name + \"/\" + subfolder[0:6]\n",
							"    return blob_service_client.get_container_client(container=new_name_for_container)\n",
							"    \n",
							"    "
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_blob_file_for_dates(blob_service_client: BlobServiceClient, container_name: str, date_start:str, date_end:str):\n",
							"    '''date_start and date_end in format YYYY-MM-DD'''\n",
							"\n",
							"    for dt in pd.period_range(start=date_start, end=date_end, freq='D'):\n",
							"        dt_str = str(dt).replace(\"-\",\"\")+'.export.CSV.zip'\n",
							"\n",
							"        container_client = _create_nested_container_client(blob_service_client, container_name, dt_str)\n",
							"        \n",
							"        send_data_to_blob(container_client,dt_str)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_last_X_days(blob_service_client: BlobServiceClient, container_name: str, days: int):\n",
							"    \n",
							"    end_date = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
							"    start_date = (datetime.today() - timedelta(days=days)).strftime(\"%Y-%m-%d\")\n",
							"\n",
							"    upload_blob_file_for_dates(blob_service_client, container_name,start_date,end_date )\n",
							"\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"bsc = get_blob_service_client_sas(account,sas_token)"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"upload_blob_file_for_dates(bsc,unizpped_container,'2023-03-24','2023-03-31')"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"upload_last_X_days(bsc,unizpped_container,7)"
						],
						"outputs": [],
						"execution_count": 24
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/configuration_paramaters_bronze_silver')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "04 Azure Synapse Analytics/configuration"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ded7acc6-3c20-45fd-909b-64b7faa87eef"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from dataclasses import dataclass\n",
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@dataclass\n",
							"class configuration_parameters_raw_bronze:\n",
							"    DATALAKE: str\n",
							"    BRONZE_CONTAINER: str\n",
							"    SILVER_CONTAINER: str\n",
							"    BRONZE_SASKEY: str\n",
							"    SILVER_SASKEY: str\n",
							"    "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"PARAMETERS = configuration_parameters_raw_bronze (\n",
							"DATALAKE=\"https://sqlday2023.blob.core.windows.net\",\n",
							"BRONZE_CONTAINER=\"bronze\",\n",
							"SILVER_CONTAINER=\"silver\",\n",
							"BRONZE_SASKEY=mssparkutils.credentials.getSecret('sqlday2023akv','SQLDayBronze','SQLDay_AKV'),\n",
							"SILVER_SASKEY=mssparkutils.credentials.getSecret('sqlday2023akv','SQLDaySilver','SQLDay_AKV')\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/configuration_paramaters_raw_bronze')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 Azure Storage/configuration"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "89b7bf73-a914-4383-a937-023bd6992bd6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from dataclasses import dataclass\n",
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@dataclass\n",
							"class configuration_parameters_raw_bronze:\n",
							"    DATALAKE: str\n",
							"    RAW_CONTAINER: str\n",
							"    BRONZE_CONTAINER: str\n",
							"    RAW_SASKEY: str\n",
							"    BRONZE_SASKEY: str\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"PARAMETERS = configuration_parameters_raw_bronze (\n",
							"DATALAKE=\"https://sqlday2023.blob.core.windows.net\",\n",
							"RAW_CONTAINER=\"raw\",\n",
							"BRONZE_CONTAINER=\"bronze\",\n",
							"RAW_SASKEY=mssparkutils.credentials.getSecret('sqlday2023akv','SQLDayRaw','SQLDay_AKV'),\n",
							"BRONZE_SASKEY=mssparkutils.credentials.getSecret('sqlday2023akv','SQLDayBronze','SQLDay_AKV')\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/configuration_parameters')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 Azure Storage/configuration"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "cf48304a-5324-4533-ae89-2bc2bd8dde47"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from dataclasses import dataclass\n",
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"\n",
							"\n",
							"@dataclass\n",
							"class configuration_parameters:\n",
							"    SRC_DATALAKE: str\n",
							"    SRC_CONTAINER: str\n",
							"    SRC_SASKEY: str\n",
							"    RAW_DATALAKE: str\n",
							"    RAW_CONTAINER: str\n",
							"    RAW_SASKEY: str\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"PARAMETERS = configuration_parameters (\n",
							"SRC_DATALAKE=\"https://itechdayadla.blob.core.windows.net\",\n",
							"SRC_CONTAINER=\"demolearn\",\n",
							"SRC_SASKEY=mssparkutils.credentials.getSecret('sqlday2023akv','DemoLearn','SQLDay_AKV'),\n",
							"RAW_DATALAKE=\"https://sqlday2023.blob.core.windows.net\",\n",
							"RAW_CONTAINER=\"raw\",\n",
							"RAW_SASKEY=mssparkutils.credentials.getSecret('sqlday2023akv','SQLDayRaw','SQLDay_AKV')\n",
							")"
						],
						"outputs": [],
						"execution_count": 4
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/configuration_parameters_akv')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 Azure Storage/configuration"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "dd0ffcf3-e963-4ac8-9e8c-e2d616e2877b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from dataclasses import dataclass\n",
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"@dataclass\n",
							"class configuration_parameters_akv:\n",
							"    DATALAKE: str\n",
							"    CONTAINER: str\n",
							"    SASKEY: str"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"PARAMETERS = configuration_parameters_akv (\n",
							"DATALAKE=\"https://itechdayadla.blob.core.windows.net\",\n",
							"CONTAINER=\"demolearn\",\n",
							"SASKEY=mssparkutils.credentials.getSecret('sqlday2023akv','DemoLearn','SQLDay_AKV')\n",
							")"
						],
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/configuration_parameters_local')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 Azure Storage/configuration"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "533497f6-6f5a-48df-8507-5e7fc200223b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from dataclasses import dataclass\n",
							"\n",
							"@dataclass\n",
							"class configuration_parameters_local:\n",
							"    DATALAKE: str\n",
							"    CONTAINER: str\n",
							"    SASKEY: str\n",
							"    URL: str\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"PARAMETERS = configuration_parameters_local (\n",
							"DATALAKE=\"https://sqlday2023.blob.core.windows.net\",\n",
							"CONTAINER=\"raw\",\n",
							"SASKEY=f'sp=racwdlmeop&st=2023-05-03T21:02:46Z&se=2023-05-31T05:02:46Z&spr=https&sv=2021-12-02&sr=c&sig=8CY7pN26Rsto5wDJoYUM4RzofvxNaozEwpZWARTqVs0%3D',\n",
							"URL= 'http://data.gdeltproject.org/events/'\n",
							")"
						],
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/configuration_parameters_local_akv')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 Azure Storage/configuration"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4d38f5c3-5e48-4942-9d14-077f52df705e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\n",
							"\n",
							"\n",
							"spark = SparkSession \\\n",
							".builder \\\n",
							".appName(\"SQLDay 2023\") \\\n",
							".getOrCreate()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"from dataclasses import dataclass\n",
							"from notebookutils import mssparkutils\n",
							"\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"@dataclass\n",
							"class configuration_parameters_local:\n",
							"    DATALAKE: str\n",
							"    CONTAINER: str\n",
							"    SASKEY: str\n",
							"    URL: str"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"PARAMETERS = configuration_parameters_local (\n",
							"DATALAKE=\"https://sqlday2023.blob.core.windows.net\",\n",
							"CONTAINER=\"raw\",\n",
							"SASKEY=mssparkutils.credentials.getSecret('sqlday2023akv','SQLDayRaw','SQLDay_AKV'),\n",
							"URL= 'http://data.gdeltproject.org/events/'\n",
							")"
						],
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/load_raw_authorization_akv')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 Azure Storage"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "90430787-7f59-4710-879f-853c6585410b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\n",
							"\n",
							"\n",
							"spark = SparkSession \\\n",
							".builder \\\n",
							".appName(\"SQLDay 2023\") \\\n",
							".getOrCreate()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
							"import requests\n",
							"import pandas as pd\n",
							"import zipfile\n",
							"import io\n",
							"from datetime import datetime, timedelta\n",
							"\n",
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run \"03 Azure Storage/configuration/configuration_parameters_local_akv\""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_configuration():\n",
							"    return PARAMETERS.DATALAKE, PARAMETERS.SASKEY, PARAMETERS.CONTAINER, PARAMETERS.URL"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_blob_service_client_sas( account: str, sas_token: str) -> BlobServiceClient:\n",
							"    account_url = account\n",
							"    credential = sas_token\n",
							"    blob_service_client = BlobServiceClient(account_url, credential=credential)\n",
							"\n",
							"    return blob_service_client"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _send_data_to_blob (container_client: ContainerClient, fileName:str):\n",
							"    url = url_base+fileName\n",
							"\n",
							"\n",
							"    print (f'url: {url}')\n",
							"\n",
							"    file = fileName.split('.')[0]\n",
							"    file_to_save = file +'.'+file_extension\n",
							"\n",
							"    r = requests.get(url, allow_redirects=True)\n",
							"    zipDocument = zipfile.ZipFile(io.BytesIO(r.content))  \n",
							"    \n",
							"    listOfFileNames = zipDocument.namelist()\n",
							"\n",
							"    for file in listOfFileNames:\n",
							"        tmp = zipDocument.extract(file)\n",
							"        print (f'file: {file}, p={tmp}')\n",
							"\n",
							"        f = open (tmp,\"r\")\n",
							"        content = f.read()\n",
							"\n",
							"        blob_client = container_client.upload_blob(name=file, data=content, overwrite=True, blob_type=\"BlockBlob\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _create_nested_container_client(blob_service_client: BlobServiceClient, container_name: str, subfolder: str) -> ContainerClient:\n",
							"    new_name_for_container = container_name + \"/\" + subfolder[0:6]\n",
							"    return blob_service_client.get_container_client(container=new_name_for_container)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_blob_file_for_dates(blob_service_client: BlobServiceClient, container_name: str, date_start:str, date_end:str):\n",
							"    '''date_start and date_end in format YYYY-MM-DD'''\n",
							"\n",
							"    for dt in pd.period_range(start=date_start, end=date_end, freq='D'):\n",
							"        dt_str = str(dt).replace(\"-\",\"\")+'.export.CSV.zip'\n",
							"\n",
							"        container_client = _create_nested_container_client(blob_service_client, container_name, dt_str)\n",
							"        \n",
							"        _send_data_to_blob(container_client,dt_str)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_last_X_days(blob_service_client: BlobServiceClient, container_name: str, days: int):\n",
							"    \n",
							"    end_date = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
							"    start_date = (datetime.today() - timedelta(days=days)).strftime(\"%Y-%m-%d\")\n",
							"\n",
							"    upload_blob_file_for_dates(blob_service_client, container_name,start_date,end_date )\n",
							""
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"account ,sas_token, container, url_base = get_configuration()\n",
							"\n",
							"file_extension = 'csv'"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"bsc = get_blob_service_client_sas(account,sas_token)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"upload_last_X_days(bsc,container,7)"
						],
						"outputs": [],
						"execution_count": 15
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/load_raw_authorization_external')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 Azure Storage"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4ba39159-d2c7-41fe-8cc8-cc3424db3221"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
							"import requests\n",
							"import os\n",
							"from datetime import datetime, timedelta\n",
							"\n",
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": 67
					},
					{
						"cell_type": "code",
						"source": [
							"%run \"03 Azure Storage/configuration/configuration_parameters\"\n",
							""
						],
						"outputs": [],
						"execution_count": 68
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _get_configuration():\n",
							"    return PARAMETERS.SRC_DATALAKE, PARAMETERS.SRC_CONTAINER, PARAMETERS.SRC_SASKEY, PARAMETERS.RAW_DATALAKE, PARAMETERS.RAW_CONTAINER, PARAMETERS.RAW_SASKEY"
						],
						"outputs": [],
						"execution_count": 69
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _get_blob_service_client( account: str, sas_token: str) -> BlobServiceClient:\n",
							"    account_url = account\n",
							"    credential = sas_token\n",
							"    blob_service_client = BlobServiceClient(account_url, credential=credential)\n",
							"\n",
							"    return blob_service_client"
						],
						"outputs": [],
						"execution_count": 70
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _get_container_client(blob_service_client: BlobServiceClient, url: str, secret: str) -> ContainerClient:\n",
							"\n",
							"    sas_key = secret\n",
							"    if '?' not in sas_key:\n",
							"        sas_key = '?'+sas_key\n",
							"    container_client = ContainerClient.from_container_url(url+sas_key)\n",
							"    return container_client"
						],
						"outputs": [],
						"execution_count": 71
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _prepare_paths_for_data_processing (src_account: str,raw_account: str, src_container: str, raw_container: str, folder: str, file_name: str):\n",
							"\n",
							"    src_account_for_abfss = src_account.split('://')[-1].replace(\"blob\",\"dfs\")\n",
							"    raw_account_for_abfss = raw_account.split('://')[-1].replace(\"blob\",\"dfs\")\n",
							"        \n",
							"    path_to_blob_src = \"abfss://\" + src_container + \"@\" + src_account_for_abfss+\"/\"+ folder +\"/\"+file_name\n",
							"    path_to_blob_raw = \"abfss://\" + raw_container + \"@\" + raw_account_for_abfss+\"/\"+ folder\n",
							"\n",
							"    return src_account_for_abfss, raw_account_for_abfss, path_to_blob_src, path_to_blob_raw"
						],
						"outputs": [],
						"execution_count": 72
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"src_account, src_container, src_sas_token, raw_account, raw_container, raw_sas_token = _get_configuration()"
						],
						"outputs": [],
						"execution_count": 73
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"source_bsc = _get_blob_service_client (src_account, src_container)"
						],
						"outputs": [],
						"execution_count": 74
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"raw_bsc = _get_blob_service_client (raw_account, raw_container)"
						],
						"outputs": [],
						"execution_count": 75
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": true
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"folder = \"Logfiles\"\n",
							"full_path_src = src_account + \"/\" + src_container \n",
							"full_path_raw = raw_account + \"/\" + raw_container + \"/\" + folder\n",
							"\n",
							"src_container_client = _get_container_client(source_bsc, full_path_src, src_sas_token)\n",
							"\n",
							"raw_container_client = _get_container_client(raw_bsc,full_path_raw, raw_sas_token )\n",
							"\n",
							"blob_list = src_container_client.list_blobs(name_starts_with=folder) \n",
							"\n",
							"\n",
							"for item in blob_list:\n",
							"    blob = os.path.basename(item.name )\n",
							"\n",
							"\n",
							"    src_account_for_abfss, raw_account_for_abfss, path_to_blob_src, path_to_blob_raw = _prepare_paths_for_data_processing(src_account,raw_account ,src_container, raw_container, folder, blob)\n",
							"    if 'json' in path_to_blob_src:\n",
							"        print (path_to_blob_src)        \n",
							"        df = spark.read.json(path_to_blob_src)\n",
							"        df.write.mode(\"append\").json(path_to_blob_raw)\n",
							"        \n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 76
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/load_raw_authorization_sas')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 Azure Storage"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8ec94ab1-dbf5-4f5d-b09a-2501429045a5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\n",
							"\n",
							"\n",
							"spark = SparkSession \\\n",
							".builder \\\n",
							".appName(\"SQLDay 2023\") \\\n",
							".getOrCreate()"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
							"import requests\n",
							"import pandas as pd\n",
							"import zipfile\n",
							"import io\n",
							"from datetime import datetime, timedelta\n",
							"\n",
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run \"03 Azure Storage/configuration/configuration_parameters_local\""
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_configuration():\n",
							"    return PARAMETERS.DATALAKE, PARAMETERS.SASKEY, PARAMETERS.CONTAINER, PARAMETERS.URL"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_blob_service_client_sas( account: str, sas_token: str) -> BlobServiceClient:\n",
							"    account_url = account\n",
							"    credential = sas_token\n",
							"    blob_service_client = BlobServiceClient(account_url, credential=credential)\n",
							"\n",
							"    return blob_service_client"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _send_data_to_blob (container_client: ContainerClient, fileName:str):\n",
							"    url = url_base+fileName\n",
							"\n",
							"\n",
							"    print (f'url: {url}')\n",
							"\n",
							"    file = fileName.split('.')[0]\n",
							"    file_to_save = file +'.'+file_extension\n",
							"\n",
							"    r = requests.get(url, allow_redirects=True)\n",
							"    zipDocument = zipfile.ZipFile(io.BytesIO(r.content))  \n",
							"    \n",
							"    listOfFileNames = zipDocument.namelist()\n",
							"\n",
							"    for file in listOfFileNames:\n",
							"        tmp = zipDocument.extract(file)\n",
							"        print (f'file: {file}, p={tmp}')\n",
							"\n",
							"        f = open (tmp,\"r\")\n",
							"        content = f.read()\n",
							"\n",
							"        blob_client = container_client.upload_blob(name=file, data=content, overwrite=True, blob_type=\"BlockBlob\")"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _create_nested_container_client(blob_service_client: BlobServiceClient, container_name: str, subfolder: str) -> ContainerClient:\n",
							"    new_name_for_container = container_name + \"/\" + subfolder[0:6]\n",
							"    return blob_service_client.get_container_client(container=new_name_for_container)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_blob_file_for_dates(blob_service_client: BlobServiceClient, container_name: str, date_start:str, date_end:str):\n",
							"    '''date_start and date_end in format YYYY-MM-DD'''\n",
							"\n",
							"    for dt in pd.period_range(start=date_start, end=date_end, freq='D'):\n",
							"        dt_str = str(dt).replace(\"-\",\"\")+'.export.CSV.zip'\n",
							"\n",
							"        container_client = _create_nested_container_client(blob_service_client, container_name, dt_str)\n",
							"        \n",
							"        _send_data_to_blob(container_client,dt_str)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_last_X_days(blob_service_client: BlobServiceClient, container_name: str, days: int):\n",
							"    \n",
							"    end_date = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
							"    start_date = (datetime.today() - timedelta(days=days)).strftime(\"%Y-%m-%d\")\n",
							"\n",
							"    upload_blob_file_for_dates(blob_service_client, container_name,start_date,end_date )\n",
							""
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"account ,sas_token, container, url_base = get_configuration()\n",
							"\n",
							"file_extension = 'csv'"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"bsc = get_blob_service_client_sas(account,sas_token)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"upload_last_X_days(bsc,container,7)"
						],
						"outputs": [],
						"execution_count": 18
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/load_raw_to_bronze_authorization_sas')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 Azure Storage"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "843d1472-052f-4b84-bce3-ea9e67255d47"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
							"import os\n",
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": 76
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run \"03 Azure Storage/configuration/configuration_paramaters_raw_bronze\""
						],
						"outputs": [],
						"execution_count": 78
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_configuration():\n",
							"    return PARAMETERS.DATALAKE, PARAMETERS.RAW_CONTAINER, PARAMETERS.BRONZE_CONTAINER, PARAMETERS.RAW_SASKEY, PARAMETERS.BRONZE_SASKEY\n",
							""
						],
						"outputs": [],
						"execution_count": 79
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_blob_service_client_sas( account: str, sas_token: str) -> BlobServiceClient:\n",
							"    account_url = account\n",
							"    credential = sas_token\n",
							"    blob_service_client = BlobServiceClient(account_url, credential=credential)\n",
							"\n",
							"    return blob_service_client"
						],
						"outputs": [],
						"execution_count": 80
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"account , raw_container, bronze_container, raw_sas_token, bronze_sas_token = get_configuration()\n",
							""
						],
						"outputs": [],
						"execution_count": 81
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"account , raw_container, bronze_container, raw_sas_token, bronze_sas_token"
						],
						"outputs": [],
						"execution_count": 82
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"raw_bsc = get_blob_service_client_sas(account,raw_sas_token)"
						],
						"outputs": [],
						"execution_count": 83
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"bronze_bsc = get_blob_service_client_sas(account,bronze_sas_token)"
						],
						"outputs": [],
						"execution_count": 84
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _get_container_client(blob_service_client: BlobServiceClient, url: str, secret: str) -> ContainerClient:\n",
							"\n",
							"    sas_key = secret\n",
							"    if '?' not in sas_key:\n",
							"        sas_key = '?'+sas_key\n",
							"    container_client = ContainerClient.from_container_url(url+sas_key)\n",
							"    return container_client"
						],
						"outputs": [],
						"execution_count": 86
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _prepare_paths_for_data_processing (account: str, raw_container: str, bronze_container: str, folder: str, file_name: str):\n",
							"\n",
							"    #abfss://raw@sqlday2023.dfs.core.windows.net/customer/part-00000-tid-3200334632332214470-9b4dec79-7e2e-495d-8657-3b5457ed3753-108-1-c000.csv\n",
							"    #abfss://bronze@sqlday2023.dfs.core.windows.net/AWorks\n",
							"\n",
							"\n",
							"    account_for_abfss = account.split('://')[-1].replace(\"blob\",\"dfs\")\n",
							"        \n",
							"    path_to_blob_raw = \"abfss://\" + raw_container + \"@\" + account_for_abfss+\"/\"+ folder +\"/\"+file_name\n",
							"    path_to_blob_bronze = \"abfss://\" + bronze_container + \"@\" + account_for_abfss+\"/\"+ folder\n",
							"    return account_for_abfss, path_to_blob_raw, path_to_blob_bronze"
						],
						"outputs": [],
						"execution_count": 87
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"folders = [\"customer\",\"orders\"]"
						],
						"outputs": [],
						"execution_count": 88
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"for folder in folders:\n",
							"    full_path_raw = account + \"/\" + raw_container \n",
							"    full_path_bronze = account + \"/\" + bronze_container + \"/\" + folder\n",
							"\n",
							"    raw_container_client = _get_container_client(raw_bsc, full_path_raw, raw_sas_token)\n",
							"    bronze_container_client = _get_container_client(raw_bsc,full_path_bronze, bronze_sas_token )\n",
							"\n",
							"    blob_list = raw_container_client.list_blobs(name_starts_with=folder) \n",
							"\n",
							"\n",
							"    for item in blob_list:\n",
							"        blob = os.path.basename(item.name )\n",
							"\n",
							"        account_for_abfss, path_to_blob_raw, path_to_blob_bronze = _prepare_paths_for_data_processing(account, raw_container, bronze_container, folder, blob)\n",
							"\n",
							"        if 'csv' in path_to_blob_raw:\n",
							"            print (path_to_blob_raw)        \n",
							"            df = spark.read.load(path_to_blob_raw, format='csv', header=True)\n",
							"            df.write.mode(\"append\").parquet(path_to_blob_bronze)\n",
							"        \n",
							""
						],
						"outputs": [],
						"execution_count": 89
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/smallpool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 3,
					"minNodeCount": 3
				},
				"nodeCount": 0,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.3",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westeurope"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sqlpool')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westeurope"
		}
	]
}