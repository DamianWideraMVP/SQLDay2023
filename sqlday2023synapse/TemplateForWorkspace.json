{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "sqlday2023synapse"
		},
		"sqlday2023synapse-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'sqlday2023synapse-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:sqlday2023synapse.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"DemoLearn_sasUri": {
			"type": "secureString",
			"metadata": "Secure string for 'sasUri' of 'DemoLearn'"
		},
		"DemoLearn_properties_typeProperties_sasToken_secretName": {
			"type": "string",
			"defaultValue": "DemoLearn"
		},
		"SQLDay_AKV_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://sqlday2023akv.vault.azure.net/"
		},
		"sqlday2023synapse-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://sqlday2023.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/01 Load Dimensions - init')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "01 Create Actor",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "01 Create Actor",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "smallpool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": true,
								"spark.dynamicAllocation.minExecutors": 1,
								"spark.dynamicAllocation.maxExecutors": 2
							},
							"driverSize": "Small",
							"numExecutors": 1
						}
					},
					{
						"name": "02 Create Geo",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "01 Create Actor",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "02 Create Geo",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "smallpool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": true,
								"spark.dynamicAllocation.minExecutors": 1,
								"spark.dynamicAllocation.maxExecutors": 2
							},
							"driverSize": "Small",
							"numExecutors": 1
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "Initial Load"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/01 Create Actor')]",
				"[concat(variables('workspaceId'), '/bigDataPools/smallpool')]",
				"[concat(variables('workspaceId'), '/notebooks/02 Create Geo')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DemoLearn')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"sasUri": "[parameters('DemoLearn_sasUri')]",
					"sasToken": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "SQLDay_AKV",
							"type": "LinkedServiceReference"
						},
						"secretName": "[parameters('DemoLearn_properties_typeProperties_sasToken_secretName')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/SQLDay_AKV')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQLDay_AKV')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('SQLDay_AKV_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sqlday2023synapse-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('sqlday2023synapse-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sqlday2023synapse-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('sqlday2023synapse-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/broze vs raw compare')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n     COUNT(*)\nFROM\n    OPENROWSET(\n        BULK 'https://sqlday2023.dfs.core.windows.net/bronze/customer/*.parquet',\n        FORMAT = 'PARQUET'\n    ) AS [result]\nGO\nSELECT\n    COUNT(*) \nFROM\n    OPENROWSET(\n        BULK 'https://sqlday2023.dfs.core.windows.net/raw/customer/*.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dedicated sql pool engineering')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT * FROM sys.schemas WHERE name = 'wwi_staging'\nGO\nCREATE SCHEMA wwi_staging\ngo\n\nCREATE TABLE [wwi_staging].[SaleHeap]\n( \n    [TransactionId] [uniqueidentifier]  NOT NULL,\n    [CustomerId] [int]  NOT NULL,\n    [ProductId] [smallint]  NOT NULL,\n    [Quantity] [smallint]  NOT NULL,\n    [Price] [decimal](9,2)  NOT NULL,\n    [TotalAmount] [decimal](9,2)  NOT NULL,\n    [TransactionDate] [int]  NOT NULL,\n    [ProfitAmount] [decimal](9,2)  NOT NULL,\n    [Hour] [tinyint]  NOT NULL,\n    [Minute] [tinyint]  NOT NULL,\n    [StoreId] [smallint]  NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = ROUND_ROBIN,\n    HEAP\n)\n\nGO\n\nCREATE TABLE [wwi_staging].[Sale]\n(\n    [TransactionId] [uniqueidentifier]  NOT NULL,\n    [CustomerId] [int]  NOT NULL,\n    [ProductId] [smallint]  NOT NULL,\n    [Quantity] [smallint]  NOT NULL,\n    [Price] [decimal](9,2)  NOT NULL,\n    [TotalAmount] [decimal](9,2)  NOT NULL,\n    [TransactionDate] [int]  NOT NULL,\n    [ProfitAmount] [decimal](9,2)  NOT NULL,\n    [Hour] [tinyint]  NOT NULL,\n    [Minute] [tinyint]  NOT NULL,\n    [StoreId] [smallint]  NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = HASH ( [CustomerId] ),\n    CLUSTERED COLUMNSTORE INDEX,\n    PARTITION\n    (\n        [TransactionDate] RANGE RIGHT FOR VALUES (20100101, 20100201, 20100301, 20100401, 20100501, 20100601, 20100701, 20100801, 20100901, 20101001, 20101101, 20101201, 20110101, 20110201, 20110301, 20110401, 20110501, 20110601, 20110701, 20110801, 20110901, 20111001, 20111101, 20111201, 20120101, 20120201, 20120301, 20120401, 20120501, 20120601, 20120701, 20120801, 20120901, 20121001, 20121101, 20121201, 20130101, 20130201, 20130301, 20130401, 20130501, 20130601, 20130701, 20130801, 20130901, 20131001, 20131101, 20131201, 20140101, 20140201, 20140301, 20140401, 20140501, 20140601, 20140701, 20140801, 20140901, 20141001, 20141101, 20141201, 20150101, 20150201, 20150301, 20150401, 20150501, 20150601, 20150701, 20150801, 20150901, 20151001, 20151101, 20151201, 20160101, 20160201, 20160301, 20160401, 20160501, 20160601, 20160701, 20160801, 20160901, 20161001, 20161101, 20161201, 20170101, 20170201, 20170301, 20170401, 20170501, 20170601, 20170701, 20170801, 20170901, 20171001, 20171101, 20171201, 20180101, 20180201, 20180301, 20180401, 20180501, 20180601, 20180701, 20180801, 20180901, 20181001, 20181101, 20181201, 20190101, 20190201, 20190301, 20190401, 20190501, 20190601, 20190701, 20190801, 20190901, 20191001, 20191101, 20191201)\n    )\n)\n\nGO\n\n\n-- load data using polybase\n\nCREATE EXTERNAL DATA SOURCE ABSS\nWITH\n( TYPE = HADOOP,\n    LOCATION = 'abfss://raw@sqlday2023.dfs.core.windows.net' \n);\n\nCREATE EXTERNAL FILE FORMAT [ParquetFormat]\nWITH (\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n)\nGO\n\nCREATE SCHEMA [wwi_external];\nGO\n\nCREATE EXTERNAL TABLE [wwi_external].Sales\n    (\n        [TransactionId] [nvarchar](36)  NOT NULL,\n        [CustomerId] [int]  NOT NULL,\n        [ProductId] [smallint]  NOT NULL,\n        [Quantity] [smallint]  NOT NULL,\n        [Price] [decimal](9,2)  NOT NULL,\n        [TotalAmount] [decimal](9,2)  NOT NULL,\n        [TransactionDate] [int]  NOT NULL,\n        [ProfitAmount] [decimal](9,2)  NOT NULL,\n        [Hour] [tinyint]  NOT NULL,\n        [Minute] [tinyint]  NOT NULL,\n        [StoreId] [smallint]  NOT NULL\n    )\nWITH\n    (\n        LOCATION = '/sale-small/Year=2019',  \n        DATA_SOURCE = ABSS,\n        FILE_FORMAT = [ParquetFormat]  \n    )  \nGO\n\n-- it takes: 26 sec\nINSERT INTO [wwi_staging].[SaleHeap]\nSELECT *\nFROM [wwi_external].[Sales]\n\n\n\n\n-- load data with copy into\nTRUNCATE TABLE wwi_staging.SaleHeap;\nGO\n\n--it takes: \nCOPY INTO wwi_staging.SaleHeap\nFROM 'https://sqlday2023.dfs.core.windows.net/raw/sale-small/Year=2019'\nWITH (\n    FILE_TYPE = 'PARQUET',\n    COMPRESSION = 'SNAPPY'\n)\nGO\n\n\n/*\nOne of the advantages COPY has over PolyBase is that it supports custom column and row delimiters.\n*/\n\n\nCREATE TABLE [wwi_staging].DailySalesCounts\n    (\n        [Date] [int]  NOT NULL,\n        [NorthAmerica] [int]  NOT NULL,\n        [SouthAmerica] [int]  NOT NULL,\n        [Europe] [int]  NOT NULL,\n        [Africa] [int]  NOT NULL,\n        [Asia] [int]  NOT NULL\n    )\nGO\n\nCOPY INTO wwi_staging.DailySalesCounts\nFROM 'https://sqlday2023.dfs.core.windows.net/raw/campaign-analytics/dailycounts.txt'\nWITH (\n    FILE_TYPE = 'CSV',\n    FIELDTERMINATOR='.',\n    ROWTERMINATOR=','\n)\nGO\n\n\nSELECT * FROM [wwi_staging].DailySalesCounts\nORDER BY [Date] DESC\n\n\n\n\n\n\n--- non standard delimiter\nCREATE EXTERNAL FILE FORMAT csv_dailysales\nWITH (\n    FORMAT_TYPE = DELIMITEDTEXT,\n    FORMAT_OPTIONS (\n        FIELD_TERMINATOR = '.',\n        DATE_FORMAT = '',\n        USE_TYPE_DEFAULT = False\n    )\n);\nGO\n\nCREATE EXTERNAL TABLE [wwi_external].DailySalesCounts\n    (\n        [Date] [int]  NOT NULL,\n        [NorthAmerica] [int]  NOT NULL,\n        [SouthAmerica] [int]  NOT NULL,\n        [Europe] [int]  NOT NULL,\n        [Africa] [int]  NOT NULL,\n        [Asia] [int]  NOT NULL\n    )\nWITH\n    (\n        LOCATION = '/campaign-analytics/dailycounts.txt',  \n        DATA_SOURCE = ABSS,\n        FILE_FORMAT = csv_dailysales\n    )  \nGO\nINSERT INTO [wwi_staging].[DailySalesCounts]\nSELECT *\nFROM [wwi_external].[DailySalesCounts]\n\n\n\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlpool",
						"poolName": "sqlpool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dedicated sql pool resource management')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- import very large parquet file\n\n/*\nThere is often a level of orchestration involved when moving data into a data warehouse, \ncoordinating movement from one or more data sources and sometimes some level of transformation. \n\nThe transformation step can occur during (extract-transform-load - ETL) or after (extract-load-transform - ELT) data movement. \nAny modern data platform must provide a seamless experience for all the typical data wrangling actions \nlike extractions, parsing, joining, standardizing, augmenting, cleansing, consolidating, and filtering. \n\nAzure Synapse Analytics provides two significant categories of features \n- data flows and data orchestrations (implemented as pipelines).\n*/\n\n\nIF EXISTS (SELECT * FROM sys.workload_management_workload_classifiers WHERE [name] = 'HeavyLoader')\nBEGIN\n    DROP WORKLOAD CLASSIFIER HeavyLoader\nEND;\n\nIF EXISTS (SELECT * FROM sys.workload_management_workload_groups WHERE name = 'BigDataLoad')\nBEGIN\n    DROP WORKLOAD GROUP BigDataLoad\nEND;\n\n--Create workload group\nCREATE WORKLOAD GROUP BigDataLoad WITH\n  (\n      MIN_PERCENTAGE_RESOURCE = 50, -- integer value\n      REQUEST_MIN_RESOURCE_GRANT_PERCENT = 25, --  (guaranteed min 4 concurrency)\n      CAP_PERCENTAGE_RESOURCE = 100\n  );\n\n-- Create workload classifier\nCREATE WORKLOAD Classifier HeavyLoader WITH\n(\n    Workload_Group ='BigDataLoad',\n    MemberName='asa.sql.import01',\n    IMPORTANCE = HIGH\n);\n\n-- View classifiers\nSELECT * FROM sys.workload_management_workload_classifiers",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlpool",
						"poolName": "sqlpool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/event source data example')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://sqlday2023.dfs.core.windows.net/bronze/202305/20230502.export.CSV',\n        FORMAT = 'CSV',\n        FIELDTERMINATOR = '\\t',\n        PARSER_VERSION = '2.0'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/serverless sql pool engineering')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://sqlday2023.dfs.core.windows.net/raw/sale-small/Year=2016/Quarter=Q3/Month=9/Day=20160930/sale-small-20160930-snappy.parquet',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n\n\nSELECT\n    TransactionDate, ProductId,\n        CAST(SUM(ProfitAmount) AS decimal(18,2)) AS [(sum) Profit],\n        CAST(AVG(ProfitAmount) AS decimal(18,2)) AS [(avg) Profit],\n        SUM(Quantity) AS [(sum) Quantity]\nFROM\n    OPENROWSET(\n        BULK 'https://sqlday2023.dfs.core.windows.net/raw/sale-small/Year=2016/Quarter=Q3/Month=9/Day=20160930/sale-small-20160930-snappy.parquet',\n        FORMAT = 'PARQUET'\n    ) AS [r] GROUP BY r.TransactionDate, r.ProductId;\n\n\n\nSELECT\n    COUNT(*)\nFROM\n    OPENROWSET(\n        BULK 'https://sqlday2023.dfs.core.windows.net/raw/sale-small/Year=2019/*/*/*/*',\n        FORMAT = 'PARQUET'\n    ) AS [r] ;\n\n\n\nGO\n\ncreate DATABASE demo_serverless\nGO\n\n\nuse demo_serverless\nGO\n\n--wyklikac external table\n\nIF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'raw_sqlday2023_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [raw_sqlday2023_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://raw@sqlday2023.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE dbo.All2019Sales (\n\t[TransactionId] nvarchar(4000),\n\t[CustomerId] int,\n\t[ProductId] smallint,\n\t[Quantity] smallint,\n\t[Price] numeric(38,18),\n\t[TotalAmount] numeric(38,18),\n\t[TransactionDate] int,\n\t[ProfitAmount] numeric(38,18),\n\t[Hour] smallint,\n\t[Minute] smallint,\n\t[StoreId] smallint\n\t)\n\tWITH (\n\tLOCATION = 'sale-small/Year=2019/*/*/*/*.parquet',\n\tDATA_SOURCE = [raw_sqlday2023_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.All2019Sales\nGO\n\n\n\n--external data \n\nIF NOT EXISTS (SELECT * FROM sys.symmetric_keys) BEGIN\n    declare @pasword nvarchar(400) = CAST(newid() as VARCHAR(400));\n    EXEC('CREATE MASTER KEY ENCRYPTION BY PASSWORD = ''' + @pasword + '''')\nEND\n\nCREATE DATABASE SCOPED CREDENTIAL [sqlondemand]\nWITH IDENTITY='SHARED ACCESS SIGNATURE',  \nSECRET = 'sv=2018-03-28&ss=bf&srt=sco&sp=rl&st=2019-10-14T12%3A10%3A25Z&se=2061-12-31T12%3A10%3A00Z&sig=KlSU2ullCscyTS0An0nozEpo4tO5JAgGBvw%2FJX2lguw%3D'\nGO\n\n-- Create external data source secured using credential\nCREATE EXTERNAL DATA SOURCE SqlOnDemandDemo WITH (\n    LOCATION = 'https://sqlondemandstorage.blob.core.windows.net',\n    CREDENTIAL = sqlondemand\n);\nGO\n\nCREATE EXTERNAL FILE FORMAT QuotedCsvWithHeader\nWITH (  \n    FORMAT_TYPE = DELIMITEDTEXT,\n    FORMAT_OPTIONS (\n        FIELD_TERMINATOR = ',',\n        STRING_DELIMITER = '\"',\n        FIRST_ROW = 2\n    )\n);\nGO\n\nCREATE EXTERNAL TABLE [population]\n(\n    [country_code] VARCHAR (5) COLLATE Latin1_General_BIN2,\n    [country_name] VARCHAR (100) COLLATE Latin1_General_BIN2,\n    [year] smallint,\n    [population] bigint\n)\nWITH (\n    LOCATION = 'csv/population/population.csv',\n    DATA_SOURCE = SqlOnDemandDemo,\n    FILE_FORMAT = QuotedCsvWithHeader\n);\nGO\n\n\n\nSELECT [country_code]\n    ,[country_name]\n    ,[year]\n    ,[population]\nFROM [dbo].[population]\nWHERE [year] = 2019 and population > 100000000\n\n\n\n\n--view \n\n\nCREATE VIEW [dbo].[Sales2019ByProduct]\n\tAS \n    SELECT [ProductId], SUM([TotalAmount]) as Total \n    FROM [dbo].[All2019Sales]\n    GROUP BY ProductId\n\n\nSELECT * FROM [dbo].[Sales2019ByProduct]\n\n\n\n--db created in spark\nUSE silverdatabase\ngo\n\nSELECT TOP (100) * FROM [dbo].[orders]\n\n\n\nCREATE VIEW dbo.UrgentOrders_View\nAS\nSELECT  O.* \nFROM [dbo].[orders] O join dbo.order_priority OP\nON O.O_ORDERPRIORITY_ID = OP.O_ORDERPRIORITY_ID\nWHERE OP.O_ORDERPRIORITY LIKE  '%URGENT'\n\n\n\nuse demo_serverless\nGO\n\nSELECT TOP (100) * FROM [silverdatabase].[dbo].[orders]\n\n\n\nCREATE VIEW dbo.UrgentOrders_View\nAS\nSELECT  O.* \nFROM [silverdatabase].[dbo].[orders] O join [silverdatabase].dbo.order_priority OP\nON O.O_ORDERPRIORITY_ID = OP.O_ORDERPRIORITY_ID\nWHERE OP.O_ORDERPRIORITY LIKE  '%URGENT'\n\n\n\nselect * from dbo.UrgentOrders_View\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "demo_serverless",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Spark job sample')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.3",
				"language": "python",
				"scanFolder": false,
				"jobProperties": {
					"name": "Spark job sample",
					"file": "abfss://synapse@sqlday2023.dfs.core.windows.net/spark_job.py",
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "98762765-218f-44b3-b6cd-94e2f81e1aca",
						"spark.synapse.context.sjdname": "Spark job sample"
					},
					"args": [
						"abfss://raw@sqlday2023.dfs.core.windows.net/AWInternetSales.csv",
						"abfss://bronze@sqlday2023.dfs.core.windows.net/AWorks"
					],
					"jars": [],
					"pyFiles": [
						""
					],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01 Create Actor')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "02 Load to Delta"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b8412392-6fc5-4594-aa99-1eb6b93d0221"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\r\n",
							"\r\n",
							"from pyspark.sql.functions import monotonically_increasing_id, col, upper, trim\r\n",
							"from delta.tables import DeltaTable"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"src_location = 'abfss://bronze@sqlday2023.dfs.core.windows.net/gdeltevents/20*/*/gdelt_events.parquet'\r\n",
							"dst_location = 'abfss://silver@sqlday2023.dfs.core.windows.net/gdeltevents/actor'"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df_all_events = spark.read.parquet(src_location)\r\n",
							"\r\n",
							"actor_columns1 = ['Actor1Code'\r\n",
							",'Actor1Name'\r\n",
							",'Actor1CountryCode'\r\n",
							",'Actor1KnownGroupCode'\r\n",
							",'Actor1EthnicCode'\r\n",
							",'Actor1Religion1Code'\r\n",
							",'Actor1Religion2Code'\r\n",
							",'Actor1Type1Code'\r\n",
							",'Actor1Type2Code'\r\n",
							",'Actor1Type3Code']\r\n",
							"actor_columns2 = ['Actor2Code'\r\n",
							",'Actor2Name'\r\n",
							",'Actor2CountryCode'\r\n",
							",'Actor2KnownGroupCode'\r\n",
							",'Actor2EthnicCode'\r\n",
							",'Actor2Religion1Code'\r\n",
							",'Actor2Religion2Code'\r\n",
							",'Actor2Type1Code'\r\n",
							",'Actor2Type2Code'\r\n",
							",'Actor2Type3Code']\r\n",
							"\r\n",
							"df_actor1 = df_all_events.select(actor_columns1)\r\n",
							"df_actor2 = df_all_events.select(actor_columns2)\r\n",
							"\r\n",
							"df_actor1 = df_actor1.toDF('ActorCode'\r\n",
							",'ActorName'\r\n",
							",'ActorCountryCode'\r\n",
							",'ActorKnownGroupCode'\r\n",
							",'ActorEthnicCode'\r\n",
							",'ActorReligion1Code'\r\n",
							",'ActorReligion2Code'\r\n",
							",'ActorType1Code'\r\n",
							",'ActorType2Code'\r\n",
							",'ActorType3Code')\r\n",
							"df_actor2 = df_actor2.toDF('ActorCode'\r\n",
							",'ActorName'\r\n",
							",'ActorCountryCode'\r\n",
							",'ActorKnownGroupCode'\r\n",
							",'ActorEthnicCode'\r\n",
							",'ActorReligion1Code'\r\n",
							",'ActorReligion2Code'\r\n",
							",'ActorType1Code'\r\n",
							",'ActorType2Code'\r\n",
							",'ActorType3Code')\r\n",
							"\r\n",
							"df_actor_all = df_actor1.union(df_actor2).dropDuplicates(['ActorCode'\r\n",
							",'ActorName'])\r\n",
							"df_actor = df_actor_all.withColumn('ActorCode', upper(trim(col('ActorCode')))).withColumn('ActorName', trim(col('ActorName')))\r\n",
							"\r\n",
							"display(df_actor.limit(10))"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"## full reload\r\n",
							"delta_table = DeltaTable.forPath(spark, dst_location)\r\n",
							"delta_table.delete()\r\n",
							"delta_table.vacuum()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"df_actor_wkey = df_actor.withColumn('ActorID', monotonically_increasing_id())\r\n",
							"df_actor_wkey.write.option(\"path\", dst_location).format(\"delta\").mode('overwrite').save()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\"\"\" df_events = df_events.toDF('GlobalEventID'\r\n",
							",'Day'\r\n",
							",'MonthYear'\r\n",
							",'Year'\r\n",
							",'FractionDate'\r\n",
							",'Actor1Code'\r\n",
							",'Actor1Name'\r\n",
							",'Actor1CountryCode'\r\n",
							",'Actor1KnownGroupCode'\r\n",
							",'Actor1EthnicCode'\r\n",
							",'Actor1Religion1Code'\r\n",
							",'Actor1Religion2Code'\r\n",
							",'Actor1Type1Code'\r\n",
							",'Actor1Type2Code'\r\n",
							",'Actor1Type3Code'\r\n",
							",'Actor2Code'\r\n",
							",'Actor2Name'\r\n",
							",'Actor2CountryCode'\r\n",
							",'Actor2KnownGroupCode'\r\n",
							",'Actor2EthnicCode'\r\n",
							",'Actor2Religion1Code'\r\n",
							",'Actor2Religion2Code'\r\n",
							",'Actor2Type1Code'\r\n",
							",'Actor2Type2Code'\r\n",
							",'Actor2Type3Code'\r\n",
							",'IsRootEvent'\r\n",
							",'EventCode'\r\n",
							",'EventBaseCode'\r\n",
							",'EventRootCode'\r\n",
							",'QuadClass'\r\n",
							",'GoldsteinScale'\r\n",
							",'NumMentions'\r\n",
							",'NumSources'\r\n",
							",'NumArticles'\r\n",
							",'AvgTone'\r\n",
							",'Actor1Geo_Type'\r\n",
							",'Actor1Geo_Fullname'\r\n",
							",'Actor1Geo_CountryCode'\r\n",
							",'Actor1Geo_ADM1Code'\r\n",
							",'Actor1GeoID'\r\n",
							",'Actor1Geo_Long'\r\n",
							",'Actor1Geo_FeatureID'\r\n",
							",'Actor2Geo_Type'\r\n",
							",'Actor2Geo_Fullname'\r\n",
							",'Actor2Geo_CountryCode'\r\n",
							",'Actor2Geo_ADM1Code'\r\n",
							",'Actor2Geo_Lat'\r\n",
							",'Actor2Geo_Long'\r\n",
							",'Actor2Geo_FeatureID'\r\n",
							",'ActionGeo_Type'\r\n",
							",'ActionGeo_Fullname'\r\n",
							",'ActionGeo_CountryCode'\r\n",
							",'ActionGeo_ADM1Code'\r\n",
							",'ActionGeo_Lat'\r\n",
							",'ActionGeo_Long'\r\n",
							",'ActionGeo_FeatureID'\r\n",
							",'DATEADDED'\r\n",
							",'SOURCEURL') \"\"\"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/02 Create Geo')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "02 Load to Delta"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "28b70292-3bb7-4dd5-8f14-f78180b78723"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\r\n",
							"\r\n",
							"from pyspark.sql.functions import monotonically_increasing_id, col, regexp_replace\r\n",
							"from delta.tables import DeltaTable"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"src_location = 'abfss://bronze@sqlday2023.dfs.core.windows.net/gdeltevents/20*/*/gdelt_events.parquet'\r\n",
							"dst_location = 'abfss://silver@sqlday2023.dfs.core.windows.net/gdeltevents/geo'"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df_all_events = spark.read.parquet(src_location)\r\n",
							"\r\n",
							"geo_columns1 = ['Actor1Geo_Type'\r\n",
							",'Actor1Geo_Fullname'\r\n",
							",'Actor1Geo_CountryCode'\r\n",
							",'Actor1Geo_ADM1Code'\r\n",
							",'Actor1Geo_Lat'\r\n",
							",'Actor1Geo_Long'\r\n",
							",'Actor1Geo_FeatureID']\r\n",
							"geo_columns2 = ['Actor2Geo_Type'\r\n",
							",'Actor2Geo_Fullname'\r\n",
							",'Actor2Geo_CountryCode'\r\n",
							",'Actor2Geo_ADM1Code'\r\n",
							",'Actor2Geo_Lat'\r\n",
							",'Actor2Geo_Long'\r\n",
							",'Actor2Geo_FeatureID']\r\n",
							"geo_columns3 = ['ActionGeo_Type'\r\n",
							",'ActionGeo_Fullname'\r\n",
							",'ActionGeo_CountryCode'\r\n",
							",'ActionGeo_ADM1Code'\r\n",
							",'ActionGeo_Lat'\r\n",
							",'ActionGeo_Long'\r\n",
							",'ActionGeo_FeatureID']\r\n",
							"\r\n",
							"df_geo1 = df_all_events.select(geo_columns1)\r\n",
							"df_geo2 = df_all_events.select(geo_columns2)\r\n",
							"df_geo3 = df_all_events.select(geo_columns3)\r\n",
							"\r\n",
							"df_geo1 = df_geo1.toDF('Geo_Type'\r\n",
							",'Geo_Fullname'\r\n",
							",'Geo_CountryCode'\r\n",
							",'Geo_ADM1Code'\r\n",
							",'Geo_Lat'\r\n",
							",'Geo_Long'\r\n",
							",'Geo_FeatureID')\r\n",
							"df_geo2 = df_geo2.toDF('Geo_Type'\r\n",
							",'Geo_Fullname'\r\n",
							",'Geo_CountryCode'\r\n",
							",'Geo_ADM1Code'\r\n",
							",'Geo_Lat'\r\n",
							",'Geo_Long'\r\n",
							",'Geo_FeatureID')\r\n",
							"df_geo3 = df_geo3.toDF('Geo_Type'\r\n",
							",'Geo_Fullname'\r\n",
							",'Geo_CountryCode'\r\n",
							",'Geo_ADM1Code'\r\n",
							",'Geo_Lat'\r\n",
							",'Geo_Long'\r\n",
							",'Geo_FeatureID')\r\n",
							"\r\n",
							"\r\n",
							"df_geo_all = df_geo1.union(df_geo2).union(df_geo3)\r\n",
							"df_geo_all = df_geo_all.withColumn('Geo_Lat', regexp_replace(col('Geo_Lat'), '[^0-9\\\\-.]', '')).withColumn('Geo_Long', regexp_replace(col('Geo_Long'), '[^0-9\\\\-.]', ''))\r\n",
							"df_geo = df_geo_all.dropDuplicates(['Geo_ADM1Code'\r\n",
							",'Geo_Lat'\r\n",
							",'Geo_Long'\r\n",
							",'Geo_FeatureID'])\r\n",
							"\r\n",
							"display(df_geo.limit(10))"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"## full reload\r\n",
							"delta_table = DeltaTable.forPath(spark, dst_location)\r\n",
							"delta_table.delete()\r\n",
							"delta_table.vacuum()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"df_geo_wkey = df_geo.withColumn('GeoID', monotonically_increasing_id())\r\n",
							"df_geo_wkey.write.option(\"path\", dst_location).format(\"delta\").mode('overwrite').save()"
						],
						"outputs": [],
						"execution_count": 5
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Data Engineering Spark Pool')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "04 Azure Synapse Analytics"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "29a12c72-8c67-48e6-a3e6-97fe16e7c0c7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": true
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
							"import os\n",
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run \"04 Azure Synapse Analytics/configuration/configuration_paramaters_bronze_silver\""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_configuration():\n",
							"    return PARAMETERS.DATALAKE, PARAMETERS.BRONZE_CONTAINER, PARAMETERS.SILVER_CONTAINER, PARAMETERS.BRONZE_SASKEY, PARAMETERS.SILVER_SASKEY\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _prepare_paths_for_data_processing (account: str, container: str, folder: str):\n",
							"\n",
							"    account_for_abfss = account.split('://')[-1].replace(\"blob\",\"dfs\")\n",
							"        \n",
							"    path_to_blob = \"abfss://\" + container + \"@\" + account_for_abfss+\"/\"+ folder\n",
							"    return account_for_abfss, path_to_blob"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"account , bronze_container, silver_container, bronze_sas_token, silver_sas_token = get_configuration()\n",
							""
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"account_for_abfss,  path_to_orders = _prepare_paths_for_data_processing(account, bronze_container, \"orders\")\n",
							"account_for_abfss,  path_to_customer = _prepare_paths_for_data_processing(account, bronze_container, \"customer\")\n",
							"account_for_abfss_silver,  path_to_customer_silver = _prepare_paths_for_data_processing(account, silver_container, \"customer/delta\")\n",
							"account_for_abfss_silver,  path_to_orders_silver = _prepare_paths_for_data_processing(account, silver_container, \"orders/delta\")\n",
							"account_for_abfss_silver,  path_to_mktsegment_silver = _prepare_paths_for_data_processing(account, silver_container, \"mktsegment/delta\")\n",
							"account_for_abfss_silver,  path_to_orderpriority_silver = _prepare_paths_for_data_processing(account, silver_container, \"orderpriority/delta\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"account_for_abfss,  path_to_orders, path_to_customer, path_to_customer_silver, path_to_orders_silver, path_to_mktsegment_silver, path_to_orderpriority_silver"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_orders = spark.read.load(path_to_orders, format='parquet')"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(df_orders.limit(10))"
						],
						"outputs": [],
						"execution_count": 165
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_customer = spark.read.load(path_to_customer, format='parquet')"
						],
						"outputs": [],
						"execution_count": 166
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(df_customer.limit(10))"
						],
						"outputs": [],
						"execution_count": 167
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import lit\n",
							"from datetime import datetime\n",
							"\n",
							"now = datetime.now()\n",
							"dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
							"\n",
							"df_customer = df_customer.withColumn(\"##_RecordLoaded\", lit(dt_string))\n",
							"df_customer = df_customer.withColumn(\"C_MKTSEGMENT_ID\", lit(0))\n",
							"\n",
							"df_orders = df_orders.withColumn(\"O_ORDERPRIORITY_ID\", lit(0))"
						],
						"outputs": [],
						"execution_count": 168
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(df_customer.limit(10))"
						],
						"outputs": [],
						"execution_count": 169
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"list_segment = df_customer.select('C_MKTSEGMENT').distinct().collect()"
						],
						"outputs": [],
						"execution_count": 170
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_mktsegment = spark.createDataFrame(list_segment)\n",
							""
						],
						"outputs": [],
						"execution_count": 171
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(df_mktsegment)"
						],
						"outputs": [],
						"execution_count": 172
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"list_order_priority = df_orders.select('O_ORDERPRIORITY').distinct().collect()\n",
							"df_order_priority= spark.createDataFrame(list_order_priority)"
						],
						"outputs": [],
						"execution_count": 173
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import functions as sf\n",
							"from pyspark.sql.window import Window\n",
							"\n",
							"df_mktsegment = df_mktsegment.withColumn(\"C_MKTSEGMENT_ID\", sf.row_number().over(Window.orderBy(sf.col(\"C_MKTSEGMENT\"))))\n",
							"df_order_priority = df_order_priority.withColumn(\"O_ORDERPRIORITY_ID\", sf.row_number().over(Window.orderBy(sf.col(\"O_ORDERPRIORITY\"))))"
						],
						"outputs": [],
						"execution_count": 174
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(df_mktsegment)"
						],
						"outputs": [],
						"execution_count": 175
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"db = \"SilverDatabase\""
						],
						"outputs": [],
						"execution_count": 176
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _drop_silver_database(db: str):\n",
							"    str = f\"DROP DATABASE IF EXISTS {db} CASCADE\"\n",
							"    spark.sql(str)\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"_drop_silver_database(db)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _create_silver_database(db: str):\n",
							"    _drop_silver_database(db)\n",
							"\n",
							"    str = f\"CREATE DATABASE IF NOT EXISTS {db}\"\n",
							"    spark.sql(str)\n",
							"\n",
							"    str = f\"USE {db}\"\n",
							"    spark.sql(str)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_mktsegment.write .format(\"delta\") .mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(path_to_mktsegment_silver)\n",
							"df_order_priority.write .format(\"delta\") .mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(path_to_orderpriority_silver)\n",
							"df_customer .write .format(\"delta\") .mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(path_to_customer_silver)\n",
							"df_orders.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(path_to_orders_silver)"
						],
						"outputs": [],
						"execution_count": 177
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_orders.write.format(\"csv\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(path_to_orders_silver)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _drop_table (folder: str):\n",
							"    \n",
							"    str = f\"DROP TABLE IF EXISTS {folder}; \"\n",
							"    spark.sql(str)\n",
							"    "
						],
						"outputs": [],
						"execution_count": 178
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"def _create_table (folder: str, path_to_silver:str):\n",
							"    \n",
							"    _drop_table(folder)\n",
							"    \n",
							"    str = f\"CREATE TABLE {folder} \\\n",
							"    USING delta \\\n",
							"    location '{path_to_silver}'\"\n",
							"\n",
							"    spark.sql(str)"
						],
						"outputs": [],
						"execution_count": 179
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"_create_table (\"mkt_segment\", path_to_mktsegment_silver)\n",
							"_create_table (\"order_priority\", path_to_orderpriority_silver)\n",
							"_create_table (\"customer\", path_to_customer_silver)\n",
							"_create_table (\"orders\", path_to_orders_silver)\n",
							""
						],
						"outputs": [],
						"execution_count": 180
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"select * from order_priority"
						],
						"outputs": [],
						"execution_count": 182
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"MERGE INTO customer dest\n",
							"USING mkt_segment src\n",
							"ON src.C_MKTSEGMENT = dest.C_MKTSEGMENT\n",
							"When Matched then\n",
							"Update \n",
							"    SET C_MKTSEGMENT_ID = src.C_MKTSEGMENT_ID\n",
							""
						],
						"outputs": [],
						"execution_count": 186
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"ALTER TABLE customer SET TBLPROPERTIES (\n",
							"   'delta.columnMapping.mode' = 'name',\n",
							"   'delta.minReaderVersion' = '2',\n",
							"   'delta.minWriterVersion' = '5')"
						],
						"outputs": [],
						"execution_count": 190
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _drop_column (table: str, column: str):\n",
							"    str = f\"ALTER TABLE {table} \\\n",
							"    DROP column {column}\"\n",
							"\n",
							"    spark.sql(str)\n",
							""
						],
						"outputs": [],
						"execution_count": 188
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"_drop_column(\"customer\",\"C_MKTSEGMENT\")"
						],
						"outputs": [],
						"execution_count": 191
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"select * from customer"
						],
						"outputs": [],
						"execution_count": 192
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"MERGE INTO orders dest\n",
							"USING order_priority src\n",
							"ON src.O_ORDERPRIORITY = dest.O_ORDERPRIORITY\n",
							"When Matched then\n",
							"Update \n",
							"    SET O_ORDERPRIORITY_ID = src.O_ORDERPRIORITY_ID"
						],
						"outputs": [],
						"execution_count": 193
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"ALTER TABLE orders SET TBLPROPERTIES (\n",
							"   'delta.columnMapping.mode' = 'name',\n",
							"   'delta.minReaderVersion' = '2',\n",
							"   'delta.minWriterVersion' = '5')"
						],
						"outputs": [],
						"execution_count": 195
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"_drop_column(\"orders\",\"O_ORDERPRIORITY\")"
						],
						"outputs": [],
						"execution_count": 196
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"some other transformations"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"class TransformationException(Exception):\n",
							"    pass"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"class MustNotModifyProtectedColumnsException (TransformationException):\n",
							"    pass"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"PROTECTED_COLUMNS: set[str] = {\n",
							"    \"##_RecordLoaded\",\n",
							"    \"_source\",\n",
							"    \"_errCode\"\n",
							"}"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _check_protected_column (column_name: str):\n",
							"    if column_name in PROTECTED_COLUMNS:\n",
							"        raise MustNotModifyProtectedColumnsException\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def add_column_to_target_as_copy (df: DataFrame, sourceColumn: str, targetColumn: str) -> DataFrame:\n",
							"    _check_protected_column(targetColumn)\n",
							"\n",
							"    if targetColumn in df.schema.names:\n",
							"        df.withColumn(column=targetColumn, col=F.col(sourceColumn))\n",
							"\n",
							"    return df"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def rename_column (df: DataFrame, before: str, after: str)-> DataFrame:\n",
							"    _check_protected_column(before)\n",
							"    _check_protected_column(after)\n",
							"\n",
							"    columns = df.columns\n",
							"\n",
							"    if before not in columns:\n",
							"        print (f'')\n",
							"    if after in columns:\n",
							"        print (f'')\n",
							"\n",
							"    return df.withColumnRenamed(before, after)    \n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def drop_column (df: DataFrame, columnName: str)-> DataFrame:\n",
							"    _check_protected_column(column_name)\n",
							"\n",
							"    return df.drop(columnName)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LoadData do not use on sqlday')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "other"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1b2c7194-d592-463b-8918-0b00d56f3e25"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install zipfile36"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\r\n",
							"import requests\r\n",
							"import pandas as pd\r\n",
							"import zipfile\r\n",
							"import io\r\n",
							"from datetime import datetime, timedelta"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sas_token = f'sp=racwdlm&st=2023-04-22T21:20:01Z&se=2023-05-31T05:20:01Z&spr=https&sv=2021-12-02&sr=c&sig=7Hb04rR6uzijvOjHdc70Xt9vyt4iEPx5i3AnoE20f%2Fg%3D'\r\n",
							"account = 'https://sqlday2023.blob.core.windows.net'\r\n",
							"url_base = 'http://data.gdeltproject.org/events/'\r\n",
							"container = 'data'\r\n",
							"unizpped_container = 'data/unizpped'\r\n",
							"file_extension = 'csv'"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_blob_service_client_sas( account, sas_token):\r\n",
							"    account_url = account\r\n",
							"    credential = sas_token\r\n",
							"    blob_service_client = BlobServiceClient(account_url, credential=credential)\r\n",
							"\r\n",
							"    return blob_service_client"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def send_data_to_blob (container_client: ContainerClient, fileName:str):\n",
							"    url = url_base+fileName\n",
							"\n",
							"\n",
							"    print (f'url: {url}')\n",
							"\n",
							"    file = fileName.split('.')[0]\n",
							"    file_to_save = file +'.'+file_extension\n",
							"\n",
							"    r = requests.get(url, allow_redirects=True)\n",
							"    zipDocument = zipfile.ZipFile(io.BytesIO(r.content))  \n",
							"    \n",
							"    listOfFileNames = zipDocument.namelist()\n",
							"\n",
							"    for file in listOfFileNames:\n",
							"        tmp = zipDocument.extract(file)\n",
							"        print (f'file: {file}, p={tmp}')\n",
							"\n",
							"        f = open (tmp,\"r\")\n",
							"        content = f.read()\n",
							"\n",
							"        blob_client = container_client.upload_blob(name=file, data=content, overwrite=True, blob_type=\"BlockBlob\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _create_nested_container_client(blob_service_client: BlobServiceClient, container_name: str, subfolder: str):\n",
							"    new_name_for_container = container_name + \"/\" + subfolder[0:6]\n",
							"    return blob_service_client.get_container_client(container=new_name_for_container)\n",
							"    \n",
							"    "
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_blob_file_1979_2005(blob_service_client: BlobServiceClient, container_name):\r\n",
							"    container_client = blob_service_client.get_container_client(container=container_name)\r\n",
							"\r\n",
							"    for dt in range(1979,2006,1):\r\n",
							"        dt_str = str(dt)+'.zip'\r\n",
							"        send_data_to_blob(container_client,dt_str)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_blob_file_200601_201303(blob_service_client: BlobServiceClient, container_name: str):\n",
							"\n",
							"    for dt in pd.period_range(start='2006-01-01', end='2013-03-01', freq='M'):\n",
							"        dt_str = str(dt).replace(\"-\",\"\")+'.zip'\n",
							"\n",
							"        container_client = _create_nested_container_client(blob_service_client, container_name, dt_str)\n",
							"        \n",
							"        send_data_to_blob(container_client,dt_str)"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_blob_file_20130401_20230331(blob_service_client: BlobServiceClient, container_name):\n",
							"    container_client = blob_service_client.get_container_client(container=container_name) \n",
							"\n",
							"    for dt in pd.period_range(start='2014-02-01', end='2023-03-31', freq='D'):\n",
							"        dt_str = str(dt).replace(\"-\",\"\")+'.export.CSV.zip'\n",
							"\n",
							"        container_client = _create_nested_container_client(blob_service_client, container_name, dt_str)\n",
							"        \n",
							"        send_data_to_blob(container_client,dt_str)"
						],
						"outputs": [],
						"execution_count": 83
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_blob_file_for_dates(blob_service_client: BlobServiceClient, container_name: str, date_start:str, date_end:str):\n",
							"    '''date_start and date_end in format YYYY-MM-DD'''\n",
							"\n",
							"    for dt in pd.period_range(start=date_start, end=date_end, freq='D'):\n",
							"        dt_str = str(dt).replace(\"-\",\"\")+'.export.CSV.zip'\n",
							"\n",
							"        container_client = _create_nested_container_client(blob_service_client, container_name, dt_str)\n",
							"        \n",
							"        send_data_to_blob(container_client,dt_str)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_last_X_days(blob_service_client: BlobServiceClient, container_name: str, days: int):\n",
							"    end_date = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
							"    start_date = (datetime.today() - timedelta(days=days)).strftime(\"%Y-%m-%d\")\n",
							"\n",
							"    upload_blob_file_for_dates(blob_service_client, container_name,start_date,end_date )\n",
							"\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"bsc = get_blob_service_client_sas(account,sas_token)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"upload_blob_file_for_dates(bsc,unizpped_container, '2023-04-20','2023-04-29')"
						],
						"outputs": [],
						"execution_count": 116
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"upload_last_X_days(bsc,unizpped_container,7)"
						],
						"outputs": [],
						"execution_count": 13
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ML on Spark Pool')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "04 Azure Synapse Analytics"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1cacbf4b-8720-4e9a-8997-47640e35d8ce"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%%configure -f\n",
							"{\n",
							"  \"name\": \"synapseml\",\n",
							"  \"conf\": {\n",
							"      \"spark.dynamicAllocation.enabled\": \"false\",\n",
							"      \"spark.jars.repositories\": \"https://mmlspark.azureedge.net/maven\"\n",
							"  }\n",
							"}"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"from synapse.ml.core.platform import *"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = (\n",
							"    spark.read.format(\"csv\")\n",
							"    .option(\"header\", True)\n",
							"    .option(\"inferSchema\", True)\n",
							"    .load(\n",
							"        \"wasbs://publicwasb@mmlspark.blob.core.windows.net/heart_disease_prediction_data.csv\"\n",
							"    )\n",
							")\n",
							"print(\"records read: \" + str(df.count()))\n",
							"print(\"Schema: \")\n",
							"df.printSchema()"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(df)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"train, test = df.randomSplit([0.85, 0.15], seed=1)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from synapse.ml.vw import VowpalWabbitFeaturizer\n",
							"\n",
							"featurizer = VowpalWabbitFeaturizer(inputCols=df.columns[:-1], outputCol=\"features\")\n",
							"train_data = featurizer.transform(train)[\"target\", \"features\"]\n",
							"test_data = featurizer.transform(test)[\"target\", \"features\"]"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(train_data.groupBy(\"target\").count())"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from synapse.ml.vw import VowpalWabbitClassifier\n",
							"\n",
							"model = VowpalWabbitClassifier(\n",
							"    numPasses=20, labelCol=\"target\", featuresCol=\"features\"\n",
							").fit(train_data)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"predictions = model.transform(test_data)\n",
							"display(predictions)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from synapse.ml.train import ComputeModelStatistics\n",
							"\n",
							"metrics = ComputeModelStatistics(\n",
							"    evaluationMetric=\"classification\", labelCol=\"target\", scoredLabelsCol=\"prediction\"\n",
							").transform(predictions)\n",
							"display(metrics)"
						],
						"outputs": [],
						"execution_count": 18
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Raw  data ingestion')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "01 Load raw data"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fe4e3443-a4b4-44a8-b9ce-de271926764e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\r\n",
							"import requests\r\n",
							"import pandas as pd\r\n",
							"import zipfile\r\n",
							"import io\r\n",
							"from datetime import datetime, timedelta"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#(bronze)sas_token = f'sp=racwdlmeop&st=2023-05-03T11:20:20Z&se=2023-05-31T19:20:20Z&spr=https&sv=2021-12-02&sr=c&sig=SITj%2FKFPa%2FTLJgawl6TeqABXjTpimz9WcH62TPyxKss%3D'\r\n",
							"sas_token = f'sp=racwdlmeop&st=2023-05-03T21:02:46Z&se=2023-05-31T05:02:46Z&spr=https&sv=2021-12-02&sr=c&sig=8CY7pN26Rsto5wDJoYUM4RzofvxNaozEwpZWARTqVs0%3D'\r\n",
							"account = 'https://sqlday2023.blob.core.windows.net'\r\n",
							"url_base = 'http://data.gdeltproject.org/events/'\r\n",
							"unizpped_container = 'raw'\r\n",
							"file_extension = 'csv'"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_blob_service_client_sas( account, sas_token):\r\n",
							"    account_url = account\r\n",
							"    credential = sas_token\r\n",
							"    blob_service_client = BlobServiceClient(account_url, credential=credential)\r\n",
							"\r\n",
							"    return blob_service_client"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def send_data_to_blob (container_client: ContainerClient, fileName:str):\n",
							"    url = url_base+fileName\n",
							"\n",
							"\n",
							"    print (f'url: {url}')\n",
							"\n",
							"    file = fileName.split('.')[0]\n",
							"    file_to_save = file +'.'+file_extension\n",
							"\n",
							"    r = requests.get(url, allow_redirects=True)\n",
							"    zipDocument = zipfile.ZipFile(io.BytesIO(r.content))  \n",
							"    \n",
							"    listOfFileNames = zipDocument.namelist()\n",
							"\n",
							"    for file in listOfFileNames:\n",
							"        tmp = zipDocument.extract(file)\n",
							"        print (f'file: {file}, p={tmp}')\n",
							"\n",
							"        f = open (tmp,\"r\")\n",
							"        content = f.read()\n",
							"\n",
							"        blob_client = container_client.upload_blob(name=file, data=content, overwrite=True, blob_type=\"BlockBlob\")"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _create_nested_container_client(blob_service_client: BlobServiceClient, container_name: str, subfolder: str):\n",
							"    new_name_for_container = container_name + \"/\" + subfolder[0:6]\n",
							"    return blob_service_client.get_container_client(container=new_name_for_container)\n",
							"    \n",
							"    "
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_blob_file_for_dates(blob_service_client: BlobServiceClient, container_name: str, date_start:str, date_end:str):\n",
							"    '''date_start and date_end in format YYYY-MM-DD'''\n",
							"\n",
							"    for dt in pd.period_range(start=date_start, end=date_end, freq='D'):\n",
							"        dt_str = str(dt).replace(\"-\",\"\")+'.export.CSV.zip'\n",
							"\n",
							"        container_client = _create_nested_container_client(blob_service_client, container_name, dt_str)\n",
							"        \n",
							"        send_data_to_blob(container_client,dt_str)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_last_X_days(blob_service_client: BlobServiceClient, container_name: str, days: int):\n",
							"    \n",
							"    end_date = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
							"    start_date = (datetime.today() - timedelta(days=days)).strftime(\"%Y-%m-%d\")\n",
							"\n",
							"    upload_blob_file_for_dates(blob_service_client, container_name,start_date,end_date )\n",
							"\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"bsc = get_blob_service_client_sas(account,sas_token)"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#upload_blob_file_for_dates(bsc,unizpped_container,'2023-03-24','2023-03-31')"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"upload_last_X_days(bsc,unizpped_container,7)"
						],
						"outputs": [],
						"execution_count": 24
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/configuration_paramaters_bronze_silver')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "04 Azure Synapse Analytics/configuration"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ded7acc6-3c20-45fd-909b-64b7faa87eef"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from dataclasses import dataclass\n",
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@dataclass\n",
							"class configuration_parameters_raw_bronze:\n",
							"    DATALAKE: str\n",
							"    BRONZE_CONTAINER: str\n",
							"    SILVER_CONTAINER: str\n",
							"    BRONZE_SASKEY: str\n",
							"    SILVER_SASKEY: str\n",
							"    "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"PARAMETERS = configuration_parameters_raw_bronze (\n",
							"DATALAKE=\"https://sqlday2023.blob.core.windows.net\",\n",
							"BRONZE_CONTAINER=\"bronze\",\n",
							"SILVER_CONTAINER=\"silver\",\n",
							"BRONZE_SASKEY=mssparkutils.credentials.getSecret('sqlday2023akv','SQLDayBronze','SQLDay_AKV'),\n",
							"SILVER_SASKEY=mssparkutils.credentials.getSecret('sqlday2023akv','SQLDaySilver','SQLDay_AKV')\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/configuration_paramaters_raw_bronze')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 Azure Storage/configuration"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "89b7bf73-a914-4383-a937-023bd6992bd6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from dataclasses import dataclass\n",
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"@dataclass\n",
							"class configuration_parameters_raw_bronze:\n",
							"    DATALAKE: str\n",
							"    RAW_CONTAINER: str\n",
							"    BRONZE_CONTAINER: str\n",
							"    RAW_SASKEY: str\n",
							"    BRONZE_SASKEY: str\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"PARAMETERS = configuration_parameters_raw_bronze (\n",
							"DATALAKE=\"https://sqlday2023.blob.core.windows.net\",\n",
							"RAW_CONTAINER=\"raw\",\n",
							"BRONZE_CONTAINER=\"bronze\",\n",
							"RAW_SASKEY=mssparkutils.credentials.getSecret('sqlday2023akv','SQLDayRaw','SQLDay_AKV'),\n",
							"BRONZE_SASKEY=mssparkutils.credentials.getSecret('sqlday2023akv','SQLDayBronze','SQLDay_AKV')\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/configuration_parameters')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 Azure Storage/configuration"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "cf48304a-5324-4533-ae89-2bc2bd8dde47"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from dataclasses import dataclass\n",
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"\n",
							"\n",
							"@dataclass\n",
							"class configuration_parameters:\n",
							"    SRC_DATALAKE: str\n",
							"    SRC_CONTAINER: str\n",
							"    SRC_SASKEY: str\n",
							"    RAW_DATALAKE: str\n",
							"    RAW_CONTAINER: str\n",
							"    RAW_SASKEY: str\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"PARAMETERS = configuration_parameters (\n",
							"SRC_DATALAKE=\"https://itechdayadla.blob.core.windows.net\",\n",
							"SRC_CONTAINER=\"demolearn\",\n",
							"SRC_SASKEY=mssparkutils.credentials.getSecret('sqlday2023akv','DemoLearn','SQLDay_AKV'),\n",
							"RAW_DATALAKE=\"https://sqlday2023.blob.core.windows.net\",\n",
							"RAW_CONTAINER=\"raw\",\n",
							"RAW_SASKEY=mssparkutils.credentials.getSecret('sqlday2023akv','SQLDayRaw','SQLDay_AKV')\n",
							")"
						],
						"outputs": [],
						"execution_count": 4
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/configuration_parameters_akv')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 Azure Storage/configuration"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "dd0ffcf3-e963-4ac8-9e8c-e2d616e2877b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from dataclasses import dataclass\n",
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"@dataclass\n",
							"class configuration_parameters_akv:\n",
							"    DATALAKE: str\n",
							"    CONTAINER: str\n",
							"    SASKEY: str"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"PARAMETERS = configuration_parameters_akv (\n",
							"DATALAKE=\"https://itechdayadla.blob.core.windows.net\",\n",
							"CONTAINER=\"demolearn\",\n",
							"SASKEY=mssparkutils.credentials.getSecret('sqlday2023akv','DemoLearn','SQLDay_AKV')\n",
							")"
						],
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/configuration_parameters_local')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 Azure Storage/configuration"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "533497f6-6f5a-48df-8507-5e7fc200223b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from dataclasses import dataclass\n",
							"\n",
							"@dataclass\n",
							"class configuration_parameters_local:\n",
							"    DATALAKE: str\n",
							"    CONTAINER: str\n",
							"    SASKEY: str\n",
							"    URL: str\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"PARAMETERS = configuration_parameters_local (\n",
							"DATALAKE=\"https://sqlday2023.blob.core.windows.net\",\n",
							"CONTAINER=\"raw\",\n",
							"SASKEY=f'sp=racwdlmeop&st=2023-05-03T21:02:46Z&se=2023-05-31T05:02:46Z&spr=https&sv=2021-12-02&sr=c&sig=8CY7pN26Rsto5wDJoYUM4RzofvxNaozEwpZWARTqVs0%3D',\n",
							"URL= 'http://data.gdeltproject.org/events/'\n",
							")"
						],
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/configuration_parameters_local_akv')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 Azure Storage/configuration"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4d38f5c3-5e48-4942-9d14-077f52df705e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\n",
							"\n",
							"\n",
							"spark = SparkSession \\\n",
							".builder \\\n",
							".appName(\"SQLDay 2023\") \\\n",
							".getOrCreate()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"from dataclasses import dataclass\n",
							"from notebookutils import mssparkutils\n",
							"\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"@dataclass\n",
							"class configuration_parameters_local:\n",
							"    DATALAKE: str\n",
							"    CONTAINER: str\n",
							"    SASKEY: str\n",
							"    URL: str"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"PARAMETERS = configuration_parameters_local (\n",
							"DATALAKE=\"https://sqlday2023.blob.core.windows.net\",\n",
							"CONTAINER=\"raw\",\n",
							"SASKEY=mssparkutils.credentials.getSecret('sqlday2023akv','SQLDayRaw','SQLDay_AKV'),\n",
							"URL= 'http://data.gdeltproject.org/events/'\n",
							")"
						],
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/load_raw_authorization_akv')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 Azure Storage"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "90430787-7f59-4710-879f-853c6585410b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\n",
							"\n",
							"\n",
							"spark = SparkSession \\\n",
							".builder \\\n",
							".appName(\"SQLDay 2023\") \\\n",
							".getOrCreate()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
							"import requests\n",
							"import pandas as pd\n",
							"import zipfile\n",
							"import io\n",
							"from datetime import datetime, timedelta\n",
							"\n",
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run \"03 Azure Storage/configuration/configuration_parameters_local_akv\""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_configuration():\n",
							"    return PARAMETERS.DATALAKE, PARAMETERS.SASKEY, PARAMETERS.CONTAINER, PARAMETERS.URL"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_blob_service_client_sas( account: str, sas_token: str) -> BlobServiceClient:\n",
							"    account_url = account\n",
							"    credential = sas_token\n",
							"    blob_service_client = BlobServiceClient(account_url, credential=credential)\n",
							"\n",
							"    return blob_service_client"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _send_data_to_blob (container_client: ContainerClient, fileName:str):\n",
							"    url = url_base+fileName\n",
							"\n",
							"\n",
							"    print (f'url: {url}')\n",
							"\n",
							"    file = fileName.split('.')[0]\n",
							"    file_to_save = file +'.'+file_extension\n",
							"\n",
							"    r = requests.get(url, allow_redirects=True)\n",
							"    zipDocument = zipfile.ZipFile(io.BytesIO(r.content))  \n",
							"    \n",
							"    listOfFileNames = zipDocument.namelist()\n",
							"\n",
							"    for file in listOfFileNames:\n",
							"        tmp = zipDocument.extract(file)\n",
							"        print (f'file: {file}, p={tmp}')\n",
							"\n",
							"        f = open (tmp,\"r\")\n",
							"        content = f.read()\n",
							"\n",
							"        blob_client = container_client.upload_blob(name=file, data=content, overwrite=True, blob_type=\"BlockBlob\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _create_nested_container_client(blob_service_client: BlobServiceClient, container_name: str, subfolder: str) -> ContainerClient:\n",
							"    new_name_for_container = container_name + \"/\" + subfolder[0:6]\n",
							"    return blob_service_client.get_container_client(container=new_name_for_container)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_blob_file_for_dates(blob_service_client: BlobServiceClient, container_name: str, date_start:str, date_end:str):\n",
							"    '''date_start and date_end in format YYYY-MM-DD'''\n",
							"\n",
							"    for dt in pd.period_range(start=date_start, end=date_end, freq='D'):\n",
							"        dt_str = str(dt).replace(\"-\",\"\")+'.export.CSV.zip'\n",
							"\n",
							"        container_client = _create_nested_container_client(blob_service_client, container_name, dt_str)\n",
							"        \n",
							"        _send_data_to_blob(container_client,dt_str)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_last_X_days(blob_service_client: BlobServiceClient, container_name: str, days: int):\n",
							"    \n",
							"    end_date = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
							"    start_date = (datetime.today() - timedelta(days=days)).strftime(\"%Y-%m-%d\")\n",
							"\n",
							"    upload_blob_file_for_dates(blob_service_client, container_name,start_date,end_date )\n",
							""
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"account ,sas_token, container, url_base = get_configuration()\n",
							"\n",
							"file_extension = 'csv'"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"bsc = get_blob_service_client_sas(account,sas_token)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"upload_last_X_days(bsc,container,7)"
						],
						"outputs": [],
						"execution_count": 15
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/load_raw_authorization_external')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 Azure Storage"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4ba39159-d2c7-41fe-8cc8-cc3424db3221"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
							"import requests\n",
							"import os\n",
							"from datetime import datetime, timedelta\n",
							"\n",
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": 67
					},
					{
						"cell_type": "code",
						"source": [
							"%run \"03 Azure Storage/configuration/configuration_parameters\"\n",
							""
						],
						"outputs": [],
						"execution_count": 68
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _get_configuration():\n",
							"    return PARAMETERS.SRC_DATALAKE, PARAMETERS.SRC_CONTAINER, PARAMETERS.SRC_SASKEY, PARAMETERS.RAW_DATALAKE, PARAMETERS.RAW_CONTAINER, PARAMETERS.RAW_SASKEY"
						],
						"outputs": [],
						"execution_count": 69
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _get_blob_service_client( account: str, sas_token: str) -> BlobServiceClient:\n",
							"    account_url = account\n",
							"    credential = sas_token\n",
							"    blob_service_client = BlobServiceClient(account_url, credential=credential)\n",
							"\n",
							"    return blob_service_client"
						],
						"outputs": [],
						"execution_count": 70
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _get_container_client(blob_service_client: BlobServiceClient, url: str, secret: str) -> ContainerClient:\n",
							"\n",
							"    sas_key = secret\n",
							"    if '?' not in sas_key:\n",
							"        sas_key = '?'+sas_key\n",
							"    container_client = ContainerClient.from_container_url(url+sas_key)\n",
							"    return container_client"
						],
						"outputs": [],
						"execution_count": 71
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _prepare_paths_for_data_processing (src_account: str,raw_account: str, src_container: str, raw_container: str, folder: str, file_name: str):\n",
							"\n",
							"    src_account_for_abfss = src_account.split('://')[-1].replace(\"blob\",\"dfs\")\n",
							"    raw_account_for_abfss = raw_account.split('://')[-1].replace(\"blob\",\"dfs\")\n",
							"        \n",
							"    path_to_blob_src = \"abfss://\" + src_container + \"@\" + src_account_for_abfss+\"/\"+ folder +\"/\"+file_name\n",
							"    path_to_blob_raw = \"abfss://\" + raw_container + \"@\" + raw_account_for_abfss+\"/\"+ folder\n",
							"\n",
							"    return src_account_for_abfss, raw_account_for_abfss, path_to_blob_src, path_to_blob_raw"
						],
						"outputs": [],
						"execution_count": 72
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"src_account, src_container, src_sas_token, raw_account, raw_container, raw_sas_token = _get_configuration()"
						],
						"outputs": [],
						"execution_count": 73
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"source_bsc = _get_blob_service_client (src_account, src_container)"
						],
						"outputs": [],
						"execution_count": 74
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"raw_bsc = _get_blob_service_client (raw_account, raw_container)"
						],
						"outputs": [],
						"execution_count": 75
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": true
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"folder = \"Logfiles\"\n",
							"full_path_src = src_account + \"/\" + src_container \n",
							"full_path_raw = raw_account + \"/\" + raw_container + \"/\" + folder\n",
							"\n",
							"src_container_client = _get_container_client(source_bsc, full_path_src, src_sas_token)\n",
							"\n",
							"raw_container_client = _get_container_client(raw_bsc,full_path_raw, raw_sas_token )\n",
							"\n",
							"blob_list = src_container_client.list_blobs(name_starts_with=folder) \n",
							"\n",
							"\n",
							"for item in blob_list:\n",
							"    blob = os.path.basename(item.name )\n",
							"\n",
							"\n",
							"    src_account_for_abfss, raw_account_for_abfss, path_to_blob_src, path_to_blob_raw = _prepare_paths_for_data_processing(src_account,raw_account ,src_container, raw_container, folder, blob)\n",
							"    if 'json' in path_to_blob_src:\n",
							"        print (path_to_blob_src)        \n",
							"        df = spark.read.json(path_to_blob_src)\n",
							"        df.write.mode(\"append\").json(path_to_blob_raw)\n",
							"        \n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 76
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/load_raw_authorization_sas')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 Azure Storage"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8ec94ab1-dbf5-4f5d-b09a-2501429045a5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\n",
							"\n",
							"\n",
							"spark = SparkSession \\\n",
							".builder \\\n",
							".appName(\"SQLDay 2023\") \\\n",
							".getOrCreate()"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
							"import requests\n",
							"import pandas as pd\n",
							"import zipfile\n",
							"import io\n",
							"from datetime import datetime, timedelta\n",
							"\n",
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run \"03 Azure Storage/configuration/configuration_parameters_local\""
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_configuration():\n",
							"    return PARAMETERS.DATALAKE, PARAMETERS.SASKEY, PARAMETERS.CONTAINER, PARAMETERS.URL"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_blob_service_client_sas( account: str, sas_token: str) -> BlobServiceClient:\n",
							"    account_url = account\n",
							"    credential = sas_token\n",
							"    blob_service_client = BlobServiceClient(account_url, credential=credential)\n",
							"\n",
							"    return blob_service_client"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _send_data_to_blob (container_client: ContainerClient, fileName:str):\n",
							"    url = url_base+fileName\n",
							"\n",
							"\n",
							"    print (f'url: {url}')\n",
							"\n",
							"    file = fileName.split('.')[0]\n",
							"    file_to_save = file +'.'+file_extension\n",
							"\n",
							"    r = requests.get(url, allow_redirects=True)\n",
							"    zipDocument = zipfile.ZipFile(io.BytesIO(r.content))  \n",
							"    \n",
							"    listOfFileNames = zipDocument.namelist()\n",
							"\n",
							"    for file in listOfFileNames:\n",
							"        tmp = zipDocument.extract(file)\n",
							"        print (f'file: {file}, p={tmp}')\n",
							"\n",
							"        f = open (tmp,\"r\")\n",
							"        content = f.read()\n",
							"\n",
							"        blob_client = container_client.upload_blob(name=file, data=content, overwrite=True, blob_type=\"BlockBlob\")"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _create_nested_container_client(blob_service_client: BlobServiceClient, container_name: str, subfolder: str) -> ContainerClient:\n",
							"    new_name_for_container = container_name + \"/\" + subfolder[0:6]\n",
							"    return blob_service_client.get_container_client(container=new_name_for_container)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_blob_file_for_dates(blob_service_client: BlobServiceClient, container_name: str, date_start:str, date_end:str):\n",
							"    '''date_start and date_end in format YYYY-MM-DD'''\n",
							"\n",
							"    for dt in pd.period_range(start=date_start, end=date_end, freq='D'):\n",
							"        dt_str = str(dt).replace(\"-\",\"\")+'.export.CSV.zip'\n",
							"\n",
							"        container_client = _create_nested_container_client(blob_service_client, container_name, dt_str)\n",
							"        \n",
							"        _send_data_to_blob(container_client,dt_str)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def upload_last_X_days(blob_service_client: BlobServiceClient, container_name: str, days: int):\n",
							"    \n",
							"    end_date = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
							"    start_date = (datetime.today() - timedelta(days=days)).strftime(\"%Y-%m-%d\")\n",
							"\n",
							"    upload_blob_file_for_dates(blob_service_client, container_name,start_date,end_date )\n",
							""
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"account ,sas_token, container, url_base = get_configuration()\n",
							"\n",
							"file_extension = 'csv'"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"bsc = get_blob_service_client_sas(account,sas_token)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"upload_last_X_days(bsc,container,7)"
						],
						"outputs": [],
						"execution_count": 18
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/load_raw_to_bronze_authorization_sas')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "03 Azure Storage"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "smallpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b3df8265-7356-4b68-9e10-5a047c7a2742"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
						"name": "smallpool",
						"type": "Spark",
						"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": true
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
							"import os\n",
							"from notebookutils import mssparkutils"
						],
						"outputs": [],
						"execution_count": 76
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run \"03 Azure Storage/configuration/configuration_paramaters_raw_bronze\""
						],
						"outputs": [],
						"execution_count": 78
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_configuration():\n",
							"    return PARAMETERS.DATALAKE, PARAMETERS.RAW_CONTAINER, PARAMETERS.BRONZE_CONTAINER, PARAMETERS.RAW_SASKEY, PARAMETERS.BRONZE_SASKEY\n",
							""
						],
						"outputs": [],
						"execution_count": 79
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_blob_service_client_sas( account: str, sas_token: str) -> BlobServiceClient:\n",
							"    account_url = account\n",
							"    credential = sas_token\n",
							"    blob_service_client = BlobServiceClient(account_url, credential=credential)\n",
							"\n",
							"    return blob_service_client"
						],
						"outputs": [],
						"execution_count": 80
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"account , raw_container, bronze_container, raw_sas_token, bronze_sas_token = get_configuration()\n",
							""
						],
						"outputs": [],
						"execution_count": 81
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"account , raw_container, bronze_container, raw_sas_token, bronze_sas_token"
						],
						"outputs": [],
						"execution_count": 82
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"raw_bsc = get_blob_service_client_sas(account,raw_sas_token)"
						],
						"outputs": [],
						"execution_count": 83
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"bronze_bsc = get_blob_service_client_sas(account,bronze_sas_token)"
						],
						"outputs": [],
						"execution_count": 84
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _get_container_client(blob_service_client: BlobServiceClient, url: str, secret: str) -> ContainerClient:\n",
							"\n",
							"    sas_key = secret\n",
							"    if '?' not in sas_key:\n",
							"        sas_key = '?'+sas_key\n",
							"    container_client = ContainerClient.from_container_url(url+sas_key)\n",
							"    return container_client"
						],
						"outputs": [],
						"execution_count": 86
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def _prepare_paths_for_data_processing (account: str, raw_container: str, bronze_container: str, folder: str, file_name: str):\n",
							"\n",
							"    #abfss://raw@sqlday2023.dfs.core.windows.net/customer/part-00000-tid-3200334632332214470-9b4dec79-7e2e-495d-8657-3b5457ed3753-108-1-c000.csv\n",
							"    #abfss://bronze@sqlday2023.dfs.core.windows.net/AWorks\n",
							"\n",
							"\n",
							"    account_for_abfss = account.split('://')[-1].replace(\"blob\",\"dfs\")\n",
							"        \n",
							"    path_to_blob_raw = \"abfss://\" + raw_container + \"@\" + account_for_abfss+\"/\"+ folder +\"/\"+file_name\n",
							"    path_to_blob_bronze = \"abfss://\" + bronze_container + \"@\" + account_for_abfss+\"/\"+ folder\n",
							"    return account_for_abfss, path_to_blob_raw, path_to_blob_bronze"
						],
						"outputs": [],
						"execution_count": 87
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"folders = [\"customer\",\"orders\"]"
						],
						"outputs": [],
						"execution_count": 88
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"for folder in folders:\n",
							"    full_path_raw = account + \"/\" + raw_container \n",
							"    full_path_bronze = account + \"/\" + bronze_container + \"/\" + folder\n",
							"\n",
							"    raw_container_client = _get_container_client(raw_bsc, full_path_raw, raw_sas_token)\n",
							"    bronze_container_client = _get_container_client(raw_bsc,full_path_bronze, bronze_sas_token )\n",
							"\n",
							"    blob_list = raw_container_client.list_blobs(name_starts_with=folder) \n",
							"\n",
							"\n",
							"    for item in blob_list:\n",
							"        blob = os.path.basename(item.name )\n",
							"\n",
							"        account_for_abfss, path_to_blob_raw, path_to_blob_bronze = _prepare_paths_for_data_processing(account, raw_container, bronze_container, folder, blob)\n",
							"\n",
							"        if 'csv' in path_to_blob_raw:\n",
							"            print (path_to_blob_raw)        \n",
							"            df = spark.read.load(path_to_blob_raw, format='csv', header=True)\n",
							"            df.write.mode(\"append\").parquet(path_to_blob_bronze)\n",
							"        \n",
							""
						],
						"outputs": [],
						"execution_count": 89
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/smallpool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 3,
					"minNodeCount": 3
				},
				"nodeCount": 0,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.3",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westeurope"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sqlpool')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westeurope"
		}
	]
}