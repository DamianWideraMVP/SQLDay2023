{
	"name": "SQLDay 2023",
	"properties": {
		"folder": {
			"name": "Sesja"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "smallpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "5c3b6db5-a6ab-4632-bf56-7549f2c3a9d6"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f173fd35-dfcd-4ef5-902e-a0345aa77c84/resourceGroups/SQLDay2023/providers/Microsoft.Synapse/workspaces/sqlday2023synapse/bigDataPools/smallpool",
				"name": "smallpool",
				"type": "Spark",
				"endpoint": "https://sqlday2023synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
					"import os\n",
					"from notebookutils import mssparkutils"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run \"Sesja/configuration/configuration_paramaters\""
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_configuration():\n",
					"    return PARAMETERS.DATALAKE, PARAMETERS.BRONZE_CONTAINER,  PARAMETERS.BRONZE_SASKEY\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def _prepare_paths_for_data_processing (account: str, container: str, folder: str):\n",
					"\n",
					"    account_for_abfss = account.split('://')[-1].replace(\"blob\",\"dfs\")\n",
					"        \n",
					"    path_to_blob = \"abfss://\" + container + \"@\" + account_for_abfss+\"/\"+ folder\n",
					"    return account_for_abfss, path_to_blob"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"account , bronze_container, bronze_sas_token = get_configuration()\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"account_for_abfss,  path_to_orders = _prepare_paths_for_data_processing(account, bronze_container, \"orders\")\n",
					"account_for_abfss,  path_to_customer = _prepare_paths_for_data_processing(account, bronze_container, \"customer\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**<mark>BROADCAST</mark>**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_ord_sch = spark.read.format(\"parquet\").option(\"header\",True).load(path_to_orders)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_cust_sch= spark.read.format(\"parquet\").option(\"header\",True).load(path_to_customer)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_ord_sch.join(df_cust_sch, df_ord_sch.O_CUSTKEY == df_cust_sch.C_CUSTKEY, \"inner\").count()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_ord_sch.join(df_cust_sch.hint(\"broadcast\"), df_ord_sch.O_CUSTKEY == df_cust_sch.C_CUSTKEY, \"inner\").count()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_ord_sch.join(df_cust_sch.hint(\"merge\"), df_ord_sch.O_CUSTKEY == df_cust_sch.C_CUSTKEY, \"inner\").count()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",2194304) #2MB 10485760b"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# MAGIC %md \n",
					"# MAGIC INPUT Partition\n",
					"\n",
					"# COMMAND ----------\n",
					"\n",
					"display(dbutils.fs.ls(\"/mnt/DemoLearn/Customer/csvFiles\"))\n",
					"\n",
					"# COMMAND ----------\n",
					"\n",
					"df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/mnt/DemoLearn/Customer/csvFiles\")\n",
					"\n",
					"# COMMAND ----------\n",
					"\n",
					"print(spark.conf.get(\"spark.sql.files.maxPartitionBytes\"))\n",
					"\n",
					"\n",
					"# COMMAND ----------\n",
					"\n",
					"df.rdd.getNumPartitions()\n",
					"\n",
					"# COMMAND ----------\n",
					"\n",
					"spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"1048576\")  \n",
					"\n",
					"# COMMAND ----------\n",
					"\n",
					"df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/mnt/DemoLearn/Customer/csvFiles\")\n",
					"df.rdd.getNumPartitions()\n",
					"\n",
					"# COMMAND ----------\n",
					"\n",
					"# MAGIC %md\n",
					"# MAGIC\n",
					"# MAGIC OUTPUT Partition\n",
					"\n",
					"# COMMAND ----------\n",
					"\n",
					"spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"134217728\") # 128 MB\n",
					"\n",
					"\n",
					"# COMMAND ----------\n",
					"\n",
					"df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/mnt/DemoLearn/Customer/csvFiles/\")\n",
					"print(f\"Number of partitions = {df.rdd.getNumPartitions()}\")\n",
					"\n",
					"\n",
					"\n",
					"# COMMAND ----------\n",
					"\n",
					"df.write.mode(\"overwrite\").option(\"path\", \"/mnt/DemoLearn/Customer/customerSingleFile/\").saveAsTable(\"customerSingleFile\")\n",
					"\n",
					"# COMMAND ----------\n",
					"\n",
					"repartitionedDF = df.repartition(8)\n",
					"print('Number of partitions: {}'.format(repartitionedDF.rdd.getNumPartitions()))\n",
					"\n",
					"# COMMAND ----------\n",
					"\n",
					"repartitionedDF = df.repartition('C_MKTSEGMENT')\n",
					"df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/mnt/DemoLearn/Customer/csvFiles/\")\n",
					"print('Number of partitions: {}'.format(repartitionedDF.rdd.getNumPartitions()))\n",
					"\n",
					"\n",
					"# COMMAND ----------\n",
					"\n",
					"coalescedDF = df.coalesce(2)\n",
					"print('Number of partitions: {}'.format(coalescedDF.rdd.getNumPartitions()))\n",
					"\n",
					"# COMMAND ----------\n",
					"\n",
					"df.write.option(\"maxRecordsPerFile\", 1000).mode(\"overwrite\").partitionBy(\"C_MKTSEGMENT\").option(\"path\", \"/mnt/DemoLearn/Customer/maxfiles/\").saveAsTable(\"customer_maxfiles\")\n",
					"\n",
					"\n",
					"\n",
					"# COMMAND ----------\n",
					"\n",
					"# MAGIC %fs ls /mnt/DemoLearn/Customer/maxfiles/C_MKTSEGMENT=BUILDING/\n",
					"\n",
					"# COMMAND ----------\n",
					"\n",
					"df_read = spark.read.format(\"parquet\").option(\"inferSchema\",\"True\").load(\"/mnt/DemoLearn/Customer/maxfiles/C_MKTSEGMENT=BUILDING/part-00000-b971999d-132e-455c-a93d-d616050b27a7.c000.snappy.parquet\")\n",
					"df_read.count()\n",
					"\n",
					"# COMMAND ----------\n",
					"\n",
					"# MAGIC %md\n",
					"# MAGIC shuffle\n",
					"\n",
					"# COMMAND ----------\n",
					"\n",
					"df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/mnt/DemoLearn/Customer/csvFiles\")\n",
					"\n",
					"# COMMAND ----------\n",
					"\n",
					"spark.conf.set(\"spark.sql.shuffle.partitions\", 200)\n",
					"mktSegmentDF = df.groupBy(\"C_MKTSEGMENT\").count().collect()\n",
					"\n",
					"# COMMAND ----------\n",
					"\n",
					"spark.conf.set(\"spark.sql.shuffle.partitions\", 30)\n",
					"mktSegmentDF = df.groupBy(\"C_MKTSEGMENT\").count().collect()\n",
					""
				],
				"execution_count": null
			}
		]
	}
}